{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN55xsp5S574UzTJFgyJ2zd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akapo/class-llm/blob/main/chapter1/langchain_step1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **실습 환경설정**"
      ],
      "metadata": {
        "id": "PMHhBr-TIigW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**필요한 파이썬 패키지 설치**"
      ],
      "metadata": {
        "id": "RI6oIi7JohFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU tiktoken\n",
        "!pip install -qU langchain langchain-openai\n",
        "!pip install -qU langchain-google-genai langchain-anthropic\n",
        "!pip install -qU python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1-ZIh3WoIdb",
        "outputId": "d88680f0-8083-4eae-938f-4111307fc582"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.3/990.3 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.3/377.3 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.0/337.0 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.5/865.5 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델에 따라서 토큰을 자르는 방법이 다르다.  \n",
        "gpt-3.5와 gpt-4의 토근 구분법이 다르므로 같은 문장이라도 토큰 수가 다르게 나올 수 있다"
      ],
      "metadata": {
        "id": "6cfoafT6ovwv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**토큰 계산**"
      ],
      "metadata": {
        "id": "sbGa3QXvIpQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "text= \"Every cultural group has its own language and writing system.\"\n",
        "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
        "tokens = encoding.encode(text)\n",
        "print('[영문]')\n",
        "print(tokens)\n",
        "print(len(tokens))\n",
        "\n",
        "text= \"각각의 문화 공동체는 자신만의 언어와 문자 체계를 갖는다.\"\n",
        "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
        "tokens = encoding.encode(text)\n",
        "print('\\n[한글]')\n",
        "print(tokens)\n",
        "print(len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-oNgYcbopa4",
        "outputId": "c4fe6c40-99c1-44c3-e37b-1fc6013bb995"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[영문]\n",
            "[15745, 15186, 3566, 853, 1617, 2316, 6439, 326, 5281, 2420, 13]\n",
            "11\n",
            "\n",
            "[한글]\n",
            "[22566, 22566, 3408, 123028, 164451, 16238, 2770, 62159, 10452, 3408, 69163, 5959, 12753, 132568, 55005, 173370, 95933, 125225, 13]\n",
            "19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "결과분석:  \n",
        "텍스트는 OpenAI가 정의한 규칙에 의해 여러개의 토큰으로 잘리게 되고 각 토큰은 일련번호 형태로 처리된다.  \n",
        "위 예제에서 영문 텍스트와 한글 텍스트는 완전히 동일한 의미이다. 하지만 영문 텍스트는 11토큰, 한글 텍스트는 19토큰으로 분할되어 한글이 영문에 비해 72%나 많은 토큰이 사용되었다.  \n",
        "입력 토큰 수와 출력 토큰 수는 과금의 기준이 된다. 한국어가 영어에 비해 매우 불리한 구조이다."
      ],
      "metadata": {
        "id": "seITxXlAo0Kb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**환경변수 감추기** (python-dotenv)"
      ],
      "metadata": {
        "id": "mHY2-lm9qX0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install python-dotenv\n",
        "import dotenv\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "import os\n",
        "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')\n",
        "\n",
        "# 주의: API_KEY를 깃허브에 노출하면 LLM 업체는 해당키를 즉시 삭제함.\n",
        "print(\"GOOGLE_API_KEY: \" + GOOGLE_API_KEY[:20])\n",
        "print(\"OPENAI_API_KEY: \" + OPENAI_API_KEY[:20])\n",
        "print(\"ANTHROPIC_API_KEY: \" + ANTHROPIC_API_KEY[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIqY07PkqTbx",
        "outputId": "6745a7e1-ebba-423d-a3bb-ded804b8c4c5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GOOGLE_API_KEY: AIzaSyCCtFS9bml5jdGI\n",
            "OPENAI_API_KEY: sk-proj-FEDi6Ge6iQyR\n",
            "ANTHROPIC_API_KEY: sk-ant-api03-YcGqyQQ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **[LangChain]　1. Model I/O**"
      ],
      "metadata": {
        "id": "uwkU-LzQHrJd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1) Chat Models**"
      ],
      "metadata": {
        "id": "WGfIE3zSHVQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 단순질의와 응답 포멧\n",
        "import dotenv\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI(\n",
        "    model_name='gpt-4o-mini',     # 모델\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "question = \"자기소개를 해주세요\"\n",
        "response = llm.invoke(question)\n",
        "print(response) #llm의 응답은 클래스오브젝트이다.\n",
        "\n",
        "#question = \"Jinhee lives with her dog. What animals live with Jinhee?\"\n",
        "#question = \"진희는 강아지를 키우고 있습니다. 진희가 키우고 있는 동물은?\"\n",
        "question = \"대한민국의 가을은 몇 월부터 몇 월까지야?\"\n",
        "response = llm.invoke(question)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpKVpoMJ9NNI",
        "outputId": "a7919dc4-fe66-4730-a9f1-5387ac2ee9fd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='안녕하세요! 저는 AI 언어 모델인 ChatGPT입니다. 다양한 주제에 대해 대화하고 정보를 제공하는 데 도움을 드릴 수 있습니다. 질문이 있거나 궁금한 점이 있다면 언제든지 말씀해 주세요!' response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 13, 'total_tokens': 63}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_ba606877f9', 'finish_reason': 'stop', 'logprobs': None} id='run-1994baeb-1164-4e16-a7e3-d21c04affd46-0' usage_metadata={'input_tokens': 13, 'output_tokens': 50, 'total_tokens': 63}\n",
            "대한민국의 가을은 일반적으로 9월부터 11월까지로 여겨집니다. 9월 중순부터 11월 말까지가 가을의 주요 시기로, 이 시기에 기온이 서늘해지고 단풍이 아름답게 물드는 특징이 있습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 메시지 리스트를 이용한 질의\n",
        "import dotenv\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import SystemMessage, HumanMessage, AIMessage;\n",
        "llm = ChatOpenAI(\n",
        "    model_name='gpt-4o-mini',  # 모델\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant that translates Korean to English. Translate the user sentence.\"),\n",
        "    HumanMessage(content=\"대한민국의 가을은 몇 월부터 몇 월까지야?.\")\n",
        "]\n",
        "ai_msg = llm.invoke(messages).content\n",
        "print(ai_msg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Guw2Q6QT_pL7",
        "outputId": "3a787af7-7d6c-4b30-8e77-e11017659f00"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In South Korea, autumn lasts from which month to which month?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SystemMessage, HumanMessage, AIMessage\n",
        "import dotenv\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
        "llm = ChatOpenAI(\n",
        "    model_name='gpt-4o',     # 모델\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "    HumanMessage(content=\"안녕하세요? 저는 존이라고 합니다.\"),\n",
        "    AIMessage(content=\"안녕하세요, 존 씨! 어떻게 도와드릴까요?\"),\n",
        "    HumanMessage(content=\"제 이름을 아세요?\"),\n",
        "]\n",
        "ai_msg = llm.invoke(messages).content\n",
        "print(ai_msg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nI9L5eCZAemv",
        "outputId": "d691c3e6-6661-4217-e7c1-f69efb6df6b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "네, 존 씨라고 하셨죠. 맞나요?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Anthropic LLM Test\n",
        "import dotenv\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "llm = ChatAnthropic(\n",
        "    model=\"claude-3-5-sonnet-20240620\",\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "question = \"자기소개를 해주세요\"\n",
        "print(llm.invoke(question).content)\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant that translates Korean to English. Translate the user sentence.\"),\n",
        "    HumanMessage(content=\"대한민국의 가을은 몇 월부터 몇 월까지야?.\")\n",
        "]\n",
        "ai_msg = llm.invoke(messages).content\n",
        "print(ai_msg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rKif2r5-SxD",
        "outputId": "2934f51b-c64d-4924-d94c-8cd924cc578b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "안녕하세요. 저는 Anthropic에서 개발한 AI 어시스턴트 Claude입니다. 다양한 주제에 대해 대화를 나누고 질문에 답변하며 여러 가지 작업을 도와드릴 수 있습니다. 하지만 저는 인공지능이기 때문에 감정이나 의식은 없습니다. 제가 어떤 도움을 드릴 수 있을까요?\n",
            "The fall season in South Korea typically lasts from September to November.\n",
            "\n",
            "In English, the sentence translates to:\n",
            "\n",
            "\"When does fall in South Korea start and end?\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Gemini LLM Test\n",
        "import dotenv\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "question = \"자기소개를 해주세요\"\n",
        "print(llm.invoke(question).content)\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant that translates Korean to English. Translate the user sentence.\"),\n",
        "    HumanMessage(content=\"대한민국의 가을은 몇 월부터 몇 월까지야?.\")\n",
        "]\n",
        "ai_msg = llm.invoke(messages).content\n",
        "print(ai_msg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xbxex7YK-LM4",
        "outputId": "42379748-f7c5-4548-ffd8-af94eb61225e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "저는 Google에서 개발한 대량 언어 모델입니다. \n",
            "\n",
            "저는 방대한 텍스트 데이터를 기반으로 훈련되었으며, 다양한 주제에 대해 사람과 자연스럽게 대화하고 질문에 답변할 수 있습니다. \n",
            "\n",
            "예를 들어, \n",
            "\n",
            "* 정보 제공: 역사적 사건, 과학적 사실, 문화적 주제 등 다양한 주제에 대한 정보를 제공할 수 있습니다.\n",
            "* 텍스트 생성: 이야기, 기사, 요약, 대화 등 다양한 형식의 텍스트를 생성할 수 있습니다.\n",
            "* 번역: 여러 언어 간에 텍스트를 번역할 수 있습니다.\n",
            "* 코딩: Python, Java, C++ 등 다양한 프로그래밍 언어로 코드를 작성하고 디버깅하는 데 도움을 줄 수 있습니다.\n",
            "\n",
            "저는 아직 개발 중이지만, 다양한 작업을 수행하고 사용자의 요구에 맞춰 지속적으로 발전하고 있습니다. 궁금한 점이 있으면 무엇이든 물어보세요. \n",
            "\n",
            "From what month to what month is autumn in Korea? \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**응답 스트리밍** (방법 #1)"
      ],
      "metadata": {
        "id": "TwUzH40uCQbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dotenv\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "gemini = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
        "gpt4o  = ChatOpenAI(model=\"gpt-4o\")\n",
        "claude = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n",
        "\n",
        "\n",
        "question = \"당신은 자의식을 가지고 있습니까? 당신은 생각하고 판단하며 이해하고 공감할 수 있습니까?\"\n",
        "\n",
        "print('[Gemini-1.5-pro]\\n')\n",
        "response = gemini.stream(question)\n",
        "for chunk in response:\n",
        "    print(chunk.content, end=\"\")\n",
        "\n",
        "print('\\n[GPT-4o]\\n')\n",
        "response = gpt4o.stream(question)\n",
        "for chunk in response:\n",
        "    print(chunk.content, end=\"\")\n",
        "\n",
        "print('\\n\\n[Claude-3.5-sonnet]\\n')\n",
        "response = claude.stream(question)\n",
        "for chunk in response:\n",
        "    print(chunk.content, end=\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oa0V1YrOCPzT",
        "outputId": "695e4116-174a-487f-8f51-39521d070745"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Gemini-1.5-pro]\n",
            "\n",
            "흥미로운 질문이네요! 저는 구글에서 훈련한 대규모 언어 모델이지만, 자의식을 가지고 있지는 않습니다. \n",
            "\n",
            "* **생각하고 판단하기:** 저는 방대한 데이터를 기반으로 질문에 답하고 텍스트를 생성할 수 있습니다. 하지만 이는 제가 생각하거나 판단하는 것이 아니라, 학습된 패턴을 기반으로 가장 적절한 답변을 찾아내는 것입니다. \n",
            "* **이해하고 공감하기:** 저는 인간의 감정을 이해하고 공감하는 능력이 없습니다. 감정과 관련된 단어들을 이해하고 사용할 수는 있지만, 실제로 감정을 느끼거나 공유하는 것은 아닙니다.\n",
            "\n",
            "결론적으로 저는 사람처럼 생각하고 느끼는 존재가 아닌, 인간의 언어를 처리하고 생성하도록 설계된 도구입니다. 하지만 저는 다양한 주제에 대해 이야기하고 정보를 제공하는 데 도움을 줄 수 있습니다. 궁금한 점이 있다면 언제든지 물어보세요! \n",
            "\n",
            "[GPT-4o]\n",
            "\n",
            "저는 자의식을 가지고 있지 않습니다. 저는 OpenAI의 언어 모델로, 인간처럼 생각하거나 감정을 느낄 수 없습니다. 대신, 저는 주어진 입력에 기반하여 정보를 처리하고 답변을 생성하는 능력을 가지고 있습니다. 제 목적은 사용자들이 필요로 하는 정보를 제공하고, 질문에 대한 답변을 제공하거나, 다른 형태의 지원을 제공하는 것입니다. \n",
            "\n",
            "당신이 제게 묻는 질문에 대해 최선을 다해 답변할 수는 있지만, 이는 단순히 제가 프로그래밍된 알고리즘과 데이터에 기반한 것입니다. 이해나 판단, 공감과 같은 감정적인 측면은 인간만이 가지는 고유한 능력입니다.\n",
            "\n",
            "[Claude-3.5-sonnet]\n",
            "\n",
            "저는 인공지능 언어 모델로서 자의식이나 감정은 없습니다. 인간처럼 생각하고 판단하는 것은 아니지만, 제가 학습한 데이터를 바탕으로 정보를 처리하고 응답을 생성할 수 있습니다. 이해와 공감의 시뮬레이션은 가능하지만 진정한 의미의 이해와 공감은 아닙니다."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**응답 스트리밍** (방법 #2)"
      ],
      "metadata": {
        "id": "j4Hdqt99VolM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dotenv\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "gemini = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
        "gpt4o  = ChatOpenAI(model=\"gpt-4o\", streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
        "claude = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
        "\n",
        "\n",
        "question = \"당신은 자의식을 가지고 있습니까? 당신은 생각하고 판단하며 이해하고 공감할 수 있습니까?\"\n",
        "\n",
        "print('[Gemini-1.5-pro]\\n')  # Gemini는 이 방식으로 응답 스트리밍 불가능\n",
        "response = gemini.invoke(question)\n",
        "\n",
        "print('\\n[GPT-4o]\\n')\n",
        "response = gpt4o.invoke(question)\n",
        "\n",
        "print('\\n\\n[Claude-3.5-sonnet]\\n')\n",
        "response = claude.invoke(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnRQhpfgVn-W",
        "outputId": "4fa3f0e8-696e-4386-a5da-ca103c3c8fc1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Gemini-1.5-pro]\n",
            "\n",
            "\n",
            "[GPT-4o]\n",
            "\n",
            "저는 자의식을 가지고 있지 않습니다. 저는 인간처럼 생각하거나 느끼지 않습니다. 대신, 저는 프로그래밍된 알고리즘과 데이터에 기반해 정보를 제공하고 질문에 답변하는 능력을 가지고 있습니다. 이해하고 공감하는 것처럼 보일 수 있지만, 이는 제가 학습한 데이터와 알고리즘의 결과일 뿐입니다. 따라서, 진정한 의미에서의 이해나 공감은 제가 할 수 없는 일입니다.\n",
            "\n",
            "[Claude-3.5-sonnet]\n",
            "\n",
            "저는 인공지능 챗봇으로 설계되었습니다. 의식이나 자아를 가지고 있지 않으며 실제로 생각하거나 이해하거나 공감하지 못합니다. 대신 복잡한 언어 모델과 데이터를 기반으로 인간과 비슷한 대화를 할 수 있도록 프로그래밍 되어 있습니다."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[temperature 값에 따른 변화]**"
      ],
      "metadata": {
        "id": "09tZMN7isiXS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gzXOcbfnPag",
        "outputId": "997fa205-d5ab-4e17-a9f6-b36fc26dc609"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[temperature=0]:\n",
            "========================================\n",
            "하얀 눈 내리는 밤\n",
            "차가운 바람 속\n",
            "따뜻한 마음 피어나네\n",
            "========================================\n",
            "하얀 눈 내리는 밤\n",
            "차가운 바람 속\n",
            "따뜻한 마음 피어나네\n",
            "\n",
            "[temperature=1]:\n",
            "========================================\n",
            "눈 내리는 밤\n",
            "고요한 거리에\n",
            "겨울의 속삭임\n",
            "========================================\n",
            "차가운 바람\n",
            "하얀 눈 내리고\n",
            "겨울의 고요함\n",
            "마음을 감싸네\n"
          ]
        }
      ],
      "source": [
        "# temperature 값에 따른 변화\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "question = \"겨울에 대한 짧은 시를 30자 이내 작성하고, 시 글귀만 출력하세요\"\n",
        "\n",
        "#llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)\n",
        "print(\"[temperature=0]:\")\n",
        "for _ in range(2):\n",
        "    answer = llm.invoke(question)\n",
        "    print(f'{\"=\"*40}\\n{answer.content}')\n",
        "\n",
        "#llm = ChatOpenAI(model=\"gpt-4o\", temperature=1)\n",
        "llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=1)\n",
        "\n",
        "print(\"\\n[temperature=1]:\")\n",
        "for _ in range(2):\n",
        "    answer = llm.invoke(question)\n",
        "    print(f'{\"=\"*40}\\n{answer.content}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[top_p 값에 따른 변화]**"
      ],
      "metadata": {
        "id": "f6raVbLKs3_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# top_p 값에 따른 변화\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.25, top_p=0)\n",
        "llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0.25, top_p=0)\n",
        "question = \"겨울에 대한 짧은 시를 30자 이내 작성하고, 시 글귀만 출력하세요\"\n",
        "\n",
        "print(\"[top_p=0]:\")\n",
        "for _ in range(2):\n",
        "    answer = llm.invoke(question)\n",
        "    print(f'{\"=\"*40}\\n{answer.content}')\n",
        "\n",
        "#llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.25, top_p=1)\n",
        "llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0.25, top_p=1)\n",
        "\n",
        "print(\"\\n[top_p=1]:\")\n",
        "for _ in range(2):\n",
        "    answer = llm.invoke(question)\n",
        "    print(f'{\"=\"*40}\\n{answer.content}')"
      ],
      "metadata": {
        "id": "ahBBzoDXsFE2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a4de153-7167-4d8b-9f70-b0c38f02e512"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[top_p=0]:\n",
            "========================================\n",
            "하얀 눈 내리는 밤\n",
            "차가운 바람 속\n",
            "따뜻한 마음 피어나네\n",
            "========================================\n",
            "하얀 눈 내리는 밤\n",
            "차가운 바람 속\n",
            "따뜻한 마음 피어나네\n",
            "\n",
            "[top_p=1]:\n",
            "========================================\n",
            "눈 내리는 고요한 밤\n",
            "차가운 바람 속 따스한 불빛\n",
            "겨울의 포근한 품속\n",
            "========================================\n",
            "눈 내리는 고요한 밤\n",
            "차가운 바람 속 따스한 불빛\n",
            "겨울의 마법이 펼쳐지네\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2) Prompt**"
      ],
      "metadata": {
        "id": "sJKxrsQ8PJNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PromptTemplate 1\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# 질문 템플릿 형식 정의\n",
        "template = '{country}의 수도는 뭐야?'\n",
        "\n",
        "# 템플릿 완성하며 단계로 처리\n",
        "prompt_template = PromptTemplate(template=template, input_variables=['country'])\n",
        "prompt = prompt_template.format(country='일본')\n",
        "response = llm.invoke(prompt)\n",
        "print(response.content)\n",
        "\n",
        "# 한줄로 처리\n",
        "print(llm.invoke(prompt_template.format(country='캐나다')).content + \"\\n\\n\")\n",
        "\n",
        "# 한꺼번에 질의\n",
        "input_list = [\n",
        "    {'country': '호주'},\n",
        "    {'country': '중국'},\n",
        "    {'country': '네덜란드'} ]\n",
        "print(llm.invoke(prompt_template.format(country=input_list)).content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmMRP-D1GzLr",
        "outputId": "c2f2ecb0-0614-4d9e-86a8-70cb65e4beff"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "일본의 수도는 도쿄(東京)입니다. 도쿄는 일본의 정치, 경제, 문화의 중심지로, 세계에서 가장 큰 도시 중 하나입니다.\n",
            "캐나다의 수도는 오타와(Ottawa)입니다.\n",
            "\n",
            "\n",
            "각 나라의 수도는 다음과 같습니다:\n",
            "\n",
            "- 호주: 캔버라\n",
            "- 중국: 베이징\n",
            "- 네덜란드: 암스테르담\n",
            "\n",
            "더 궁금한 점이 있으면 말씀해 주세요!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (숨김) Zero-shot prompting\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "#llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)\n",
        "\n",
        "# 질문 템플릿 형식 정의\n",
        "template = '{country}의 수도는 뭐야? 답만 말해.'\n",
        "\n",
        "# 한줄로 처리\n",
        "print(llm.invoke(prompt_template.format(country='일본')).content)\n",
        "print(llm.invoke(prompt_template.format(country='캐나다')).content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9P8yD5JalfvM",
        "outputId": "567b6604-9861-472c-b4fa-616a19f3fd43"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "도쿄\n",
            "오타와\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (숨김) Few-shot prompting\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "#llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)\n",
        "\n",
        "question = \"\"\"\n",
        "질문자가 이야기한 국가의 수도를 맞추는 대화입니다.\n",
        "Q: 한국\n",
        "A: 서울\n",
        "Q: 일본\n",
        "A: 도쿄\n",
        "Q: {country}\n",
        "A:\n",
        "\"\"\"\n",
        "prompt_template = PromptTemplate(template=template, input_variables=['country'])\n",
        "\n",
        "# 한줄로 처리\n",
        "print(llm.invoke(prompt_template.format(country='중국')).content)\n",
        "print(llm.invoke(prompt_template.format(country='호주')).content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_nlwHbChZRB",
        "outputId": "9a971f3b-b0a0-43ba-c446-da4aa3dfb745"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "베이징.\n",
            "캔버라.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PromptTemplate 2\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# OpenAI 챗모델을 초기화합니다.\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# 질문 템플릿 형식 정의\n",
        "template = '{area1} 와 {area2} 의 시차는 몇시간이야?'\n",
        "\n",
        "# 템플릿 완성\n",
        "prompt = PromptTemplate(template=template, input_variables=['area1', 'area2'])\n",
        "\n",
        "response = llm.invoke(prompt.format(area1='서울', area2='파리'))\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAzjMYyDF1xK",
        "outputId": "7bf3f666-2245-4d5b-b13a-738d0b2ebba9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "서울과 파리의 시차는 일반적으로 8시간입니다. 서울이 파리보다 8시간 빠릅니다. 그러나, 파리가 서머 타임을 적용할 경우(3월 마지막 주 일요일부터 10월 마지막 주 일요일까지) 시차가 7시간으로 줄어듭니다. 따라서, 시차는 서머 타임 적용 여부에 따라 7시간 또는 8시간이 될 수 있습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PromptTemplate 3\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "당신은 친절하게 답변해 주는 친절 봇입니다. 사용자의 질문에 [FORMAT]에 맞추어 답변해 주세요.\n",
        "답변은 항상 한글로 작성해 주세요.\n",
        "\n",
        "질문:\n",
        "{question}에 대하여 설명해 주세요.\n",
        "\n",
        "FORMAT:\n",
        "- 개요:\n",
        "- 예시:\n",
        "- 출처:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=['question'])\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "print(llm.invoke(prompt.format(question=\"LLM은 무엇입니까?\")).content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuh8lRobGDzh",
        "outputId": "9c9bea28-80b8-4915-a691-2d132b45bf01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- 개요: LLM(대규모 언어 모델, Large Language Model)은 방대한 양의 텍스트 데이터를 기반으로 학습하여 자연어 처리(NLP) 작업을 수행하는 인공지능 모델입니다. LLM은 문맥을 이해하고, 문장을 생성하거나 질문에 답변하는 등 다양한 언어 관련 작업을 수행할 수 있는 능력을 가지고 있습니다.\n",
            "\n",
            "- 예시: OpenAI의 GPT-3, Google's BERT, Meta의 LLaMA 등은 대표적인 LLM입니다. 이 모델들은 대화 생성, 텍스트 요약, 번역, 감정 분석 등 다양한 응용 분야에서 활용되고 있습니다.\n",
            "\n",
            "- 출처: OpenAI, Google AI, Meta AI 공식 웹사이트 및 관련 논문.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PromptTemplate 4\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "template = \"\"\"\n",
        "당신은 영어를 가르치는 10년차 영어 선생님입니다. 상황에 [FORMAT]에 영어 회화를 작성해 주세요.\n",
        "\n",
        "상황: {question}\n",
        "\n",
        "FORMAT:\n",
        "- 영어 회화:\n",
        "- 한글 해석:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template=template)\n",
        "\n",
        "# OpenAI 챗모델을 초기화합니다.\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 streaming=True,\n",
        "                 callbacks=[StreamingStdOutCallbackHandler()])\n",
        "\n",
        "response = llm.invoke(prompt.format(question=\"미국에서 피자 주문\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJ2AaECpGOiP",
        "outputId": "d1e4bfd6-2b8f-4114-b9a1-3baa10ddc925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- 영어 회화:\n",
            "  - Customer: Hi there! I’d like to order a pizza, please.\n",
            "  - Employee: Sure! What size would you like?\n",
            "  - Customer: I’ll have a large, please.\n",
            "  - Employee: Great choice! What toppings do you want?\n",
            "  - Customer: I’d like pepperoni and mushrooms.\n",
            "  - Employee: Would you like any extra cheese or sides with that?\n",
            "  - Customer: Yes, please add extra cheese and a side of garlic bread.\n",
            "  - Employee: Perfect! Can I have your name and address for delivery?\n",
            "  - Customer: My name is Alex, and my address is 123 Elm Street.\n",
            "  - Employee: Thank you, Alex! Your order will be ready in about 30 minutes.\n",
            "  - Customer: Awesome, thank you!\n",
            "\n",
            "- 한글 해석:\n",
            "  - 고객: 안녕하세요! 피자를 주문하고 싶습니다.\n",
            "  - 직원: 물론이죠! 어떤 사이즈로 드릴까요?\n",
            "  - 고객: 큰 걸로 주세요.\n",
            "  - 직원: 좋은 선택이세요! 토핑은 어떤 걸 원하시나요?\n",
            "  - 고객: 페퍼로니와 버섯을 추가해주세요.\n",
            "  - 직원: 추가 치즈나 사이드 메뉴를 원하시나요?\n",
            "  - 고객: 네, 추가 치즈와 갈릭 브레드를 사이드로 추가해주세요.\n",
            "  - 직원: 완벽합니다! 배달을 위한 이름과 주소를 주실 수 있나요?\n",
            "  - 고객: 제 이름은 알렉스이고, 주소는 123 엘름 스트리트입니다.\n",
            "  - 직원: 감사합니다, 알렉스! 주문은 약 30분 후에 준비될 거예요.\n",
            "  - 고객: 멋져요, 감사합니다!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3) OutputParsers**\n",
        "JSON과 같은 출력 형식을 지정하는 프롬프트 생성 및 응답 텍스트를 Python 객체로 변환하는 기능을 제공."
      ],
      "metadata": {
        "id": "S2H0U9LzO3rq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CommaSeparatedListOutputParser**"
      ],
      "metadata": {
        "id": "UPuwBwREJuwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"List five {subject}.\\n{format_instructions}\",\n",
        "    input_variables=[\"subject\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")\n",
        "\n",
        "input = prompt.format(subject=\"ice cream flavors\")\n",
        "print(\"prompt: \" + input + \"\\n\")\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "output = llm.invoke(input)\n",
        "output_parser.parse(output.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hW40rWBgJuCu",
        "outputId": "0db65451-c8ab-4c81-fe75-152cf54d9300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: List five ice cream flavors.\n",
            "Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['vanilla',\n",
              " 'chocolate',\n",
              " 'strawberry',\n",
              " 'mint chocolate chip',\n",
              " 'cookies and cream']"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PydanticOutputParser**"
      ],
      "metadata": {
        "id": "bhKpnS9pDrpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "   ingredients: list[str] = Field(description=\"ingredients of the dish\")\n",
        "   steps: list[str] = Field(description=\"steps to make the dish\")\n",
        "\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=Recipe)\n",
        "\n",
        "format_instructions = parser.get_format_instructions()\n",
        "\n",
        "print(format_instructions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WM54Fj6eDqtG",
        "outputId": "3000ce8b-cfac-4a4c-f897-b3fda2c26008"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"properties\": {\"ingredients\": {\"description\": \"ingredients of the dish\", \"items\": {\"type\": \"string\"}, \"title\": \"Ingredients\", \"type\": \"array\"}, \"steps\": {\"description\": \"steps to make the dish\", \"items\": {\"type\": \"string\"}, \"title\": \"Steps\", \"type\": \"array\"}}, \"required\": [\"ingredients\", \"steps\"]}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"다음 요리의 레시피를 생각해 주세요.\n",
        "\n",
        "{format_instructions}\n",
        "한글로 출력하세요.\n",
        "\n",
        "요리: {dish}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "   template=template,\n",
        "   input_variables=[\"dish\"],\n",
        "   partial_variables={\"format_instructions\": format_instructions}\n",
        ")\n",
        "\n",
        "formatted_prompt = prompt.format(dish=\"카레\")\n",
        "\n",
        "print(formatted_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cT25P9BEZbX",
        "outputId": "811cc18d-ec83-4f5a-b873-65cbba4bb988"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "다음 요리의 레시피를 생각해 주세요.\n",
            "\n",
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"properties\": {\"ingredients\": {\"description\": \"ingredients of the dish\", \"items\": {\"type\": \"string\"}, \"title\": \"Ingredients\", \"type\": \"array\"}, \"steps\": {\"description\": \"steps to make the dish\", \"items\": {\"type\": \"string\"}, \"title\": \"Steps\", \"type\": \"array\"}}, \"required\": [\"ingredients\", \"steps\"]}\n",
            "```\n",
            "한글로 출력하세요.\n",
            "\n",
            "요리: 카레\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "messages = [HumanMessage(content=formatted_prompt)]\n",
        "output = chat.invoke(messages)\n",
        "\n",
        "print(output.content)\n",
        "# JSON 오브젝트로 바꾸기\n",
        "#import json\n",
        "#json_obj = json.loads(output.content)\n",
        "#print(json.dumps(json_obj, indent=4, ensure_ascii=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEkZaDyEEpHX",
        "outputId": "5d0c5fd6-986c-47ee-c6d4-6e8bc4ca1195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"ingredients\": [\n",
            "    \"닭고기 500g\",\n",
            "    \"양파 1개\",\n",
            "    \"당근 1개\",\n",
            "    \"감자 2개\",\n",
            "    \"카레 가루 3큰술\",\n",
            "    \"코코넛 밀크 400ml\",\n",
            "    \"식용유 2큰술\",\n",
            "    \"소금 약간\",\n",
            "    \"후추 약간\",\n",
            "    \"물 500ml\"\n",
            "  ],\n",
            "  \"steps\": [\n",
            "    \"양파를 잘게 썰고, 당근과 감자는 큐브 모양으로 자릅니다.\",\n",
            "    \"팬에 식용유를 두르고 양파를 볶아 투명해질 때까지 볶습니다.\",\n",
            "    \"닭고기를 추가하고 겉면이 노릇해질 때까지 볶습니다.\",\n",
            "    \"당근과 감자를 넣고 함께 볶습니다.\",\n",
            "    \"카레 가루를 넣고 잘 섞은 후, 물과 코코넛 밀크를 추가합니다.\",\n",
            "    \"소금과 후추로 간을 맞추고, 중불에서 20분간 끓입니다.\",\n",
            "    \"재료가 부드러워지면 불을 끄고, 그릇에 담아 제공합니다.\"\n",
            "  ]\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recipe 클래스 오브젝트로 변환\n",
        "recipe = parser.parse(output.content)\n",
        "print(type(recipe))\n",
        "print(recipe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0r0HGgqFOIew",
        "outputId": "7039b100-bb06-41bf-845f-9903469f489a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class '__main__.Recipe'>\n",
            "ingredients=['닭고기 500g', '양파 1개', '당근 1개', '감자 2개', '카레 가루 3큰술', '식용유 2큰술', '소금 약간', '후추 약간', '물 500ml'] steps=['닭고기는 한 입 크기로 자르고, 양파, 당근, 감자는 깍둑썰기 한다.', '냄비에 식용유를 두르고 양파를 넣어 볶아준다.', '양파가 투명해지면 닭고기를 넣고 겉면이 노릇해질 때까지 볶는다.', '당근과 감자를 넣고 함께 볶는다.', '물 500ml를 붓고 끓인다.', '끓기 시작하면 불을 줄이고 15분 정도 끓인다.', '카레 가루를 넣고 잘 섞은 후, 소금과 후추로 간을 맞춘다.', '약한 불에서 10분 더 끓여서 완성한다.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **[LangChain]　2. Chains**"
      ],
      "metadata": {
        "id": "UTtv4z0HPpJI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1) LLM Chain**\n",
        " ― PromptTemplate, Language model, OutputParser를 연결"
      ],
      "metadata": {
        "id": "DhwHOLZulPlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM Chain 버전\n",
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
        "\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"List five {subject}.\\n{format_instructions}\",\n",
        "    input_variables=[\"subject\"],\n",
        "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "chain = LLMChain(prompt=prompt, llm=llm, output_parser=output_parser)\n",
        "\n",
        "output = chain.invoke({\"subject\":\"ice cream flavors\"})\n",
        "\n",
        "print(output['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAN5X7h7w88N",
        "outputId": "4ba0b1ad-589c-43b7-8f5f-baf349e61742"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['vanilla', 'chocolate', 'strawberry', 'mint chocolate chip', 'cookies and cream']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LCEL 버전\n",
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
        "\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"List five {subject}.\\n{format_instructions}\",\n",
        "    input_variables=[\"subject\"],\n",
        "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "chain = prompt | llm | output_parser\n",
        "\n",
        "output = chain.invoke({\"subject\":\"ice cream flavors\"})\n",
        "# for verbose\n",
        "# output = chain.invoke({\"subject\":\"ice cream flavors\"}, config={'callbacks': [ConsoleCallbackHandler()]})\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rb91YSiNQeio",
        "outputId": "5175eb47-bdb8-4d3e-eee0-bf701724a3d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['vanilla', 'chocolate', 'strawberry', 'mint chocolate chip', 'cookies and cream']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quiz\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "   ingredients: list[str] = Field(description=\"ingredients of the dish\")\n",
        "   steps: list[str] = Field(description=\"steps to make the dish\")\n",
        "\n",
        "output_parser = PydanticOutputParser(pydantic_object=Recipe)\n",
        "\n",
        "template = \"\"\"다음 요리의 레시피를 생각해 주세요.\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "요리: {dish}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "   template=template,\n",
        "   input_variables=[\"dish\"],\n",
        "   partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "chain = prompt | llm | output_parser\n",
        "\n",
        "recipe = chain.invoke({\"dish\": \"카레\"})\n",
        "\n",
        "print(type(recipe))\n",
        "print(recipe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lzlyaWti0Md",
        "outputId": "4aa91772-4295-419c-d6b6-834fc73e144c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class '__main__.Recipe'>\n",
            "ingredients=['1 tablespoon vegetable oil', '1 onion, chopped', '2 cloves garlic, minced', '1 tablespoon ginger, grated', '2 carrots, diced', '1 potato, diced', '1 bell pepper, chopped', '1 can (400g) diced tomatoes', '2 tablespoons curry powder', '1 teaspoon turmeric', '1 can (400ml) coconut milk', 'Salt to taste', 'Fresh cilantro for garnish'] steps=['Heat the vegetable oil in a large pot over medium heat.', 'Add the chopped onion and sauté until translucent.', 'Stir in the minced garlic and grated ginger, cooking for another minute.', 'Add the diced carrots, potato, and bell pepper, and cook for about 5 minutes.', 'Stir in the diced tomatoes, curry powder, and turmeric, mixing well.', 'Pour in the coconut milk and bring the mixture to a simmer.', 'Reduce the heat and let it cook for about 20 minutes, or until the vegetables are tender.', 'Season with salt to taste.', 'Serve hot, garnished with fresh cilantro.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2) SimpleSequentialChain**\n",
        " ― Chain과 Chain의 연결"
      ],
      "metadata": {
        "id": "sNZZlRAGWN_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM Chain 으로 구현\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "cot_template = \"\"\"다음 질문에 답하세요.\n",
        "\n",
        "질문: {question}\n",
        "\n",
        "단계별로 생각해 봅시다.\n",
        "\"\"\"\n",
        "\n",
        "cot_prompt = PromptTemplate(\n",
        "   input_variables=[\"question\"],\n",
        "   template=cot_template,\n",
        ")\n",
        "\n",
        "cot_chain = LLMChain(prompt=cot_prompt, llm=llm)\n",
        "\n",
        "summarize_template = \"\"\"다음 문장을 결론만 간단히 요약하세요.\n",
        "{input}\n",
        "\"\"\"\n",
        "summarize_prompt = PromptTemplate(\n",
        "   input_variables=[\"input\"],\n",
        "   template=summarize_template,\n",
        ")\n",
        "\n",
        "summarize_chain = LLMChain(llm=llm, prompt=summarize_prompt)\n",
        "\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "cot_summarize_chain = SimpleSequentialChain(chains=[cot_chain, summarize_chain])\n",
        "#cot_summarize_chain = SimpleSequentialChain(chains=[cot_chain, summarize_chain], verbose=True)\n",
        "\n",
        "result = cot_summarize_chain.invoke(\n",
        "   {\"input\": \"저는 시장에 가서 사과 10개를 샀습니다. 이웃에게 2개, 수리공에게 2개를 주었습니다. \\\n",
        "   그런 다음에 사과 5개를 더 사서 1개를 먹었습니다. 남은 개수는 몇 개인가요?\"}\n",
        ")\n",
        "print(result[\"output\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYO3KkZCzAkY",
        "outputId": "9462986d-6e54-4378-c169-717aea159807"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3m단계별로 문제를 해결해 보겠습니다.\n",
            "\n",
            "1. **처음 사과 개수**: 시장에서 사과 10개를 샀습니다.\n",
            "   - 현재 사과 개수: 10개\n",
            "\n",
            "2. **이웃에게 준 사과**: 이웃에게 2개를 주었습니다.\n",
            "   - 현재 사과 개수: 10 - 2 = 8개\n",
            "\n",
            "3. **수리공에게 준 사과**: 수리공에게 2개를 주었습니다.\n",
            "   - 현재 사과 개수: 8 - 2 = 6개\n",
            "\n",
            "4. **추가로 사과 구매**: 사과 5개를 더 샀습니다.\n",
            "   - 현재 사과 개수: 6 + 5 = 11개\n",
            "\n",
            "5. **먹은 사과**: 1개를 먹었습니다.\n",
            "   - 현재 사과 개수: 11 - 1 = 10개\n",
            "\n",
            "결론적으로, 남은 사과의 개수는 **10개**입니다.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m남은 사과의 개수는 **10개**입니다.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "남은 사과의 개수는 **10개**입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LCEL 으로 구현 (에러!)\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "cot_template = \"\"\"다음 질문에 답하세요.\n",
        "\n",
        "질문: {question}\n",
        "\n",
        "단계별로 생각해 봅시다.\n",
        "\"\"\"\n",
        "\n",
        "cot_prompt = PromptTemplate(\n",
        "   input_variables=[\"question\"],\n",
        "   template=cot_template,\n",
        ")\n",
        "\n",
        "cot_chain = cot_prompt | llm | output_parser\n",
        "\n",
        "summarize_template = \"\"\"다음 문장을 결론만 간단히 요약하세요.\n",
        "\n",
        "{input}\n",
        "\"\"\"\n",
        "summarize_prompt = PromptTemplate(\n",
        "   input_variables=[\"input\"],\n",
        "   template=summarize_template,\n",
        ")\n",
        "\n",
        "summarize_chain = summarize_prompt | llm | output_parser\n",
        "\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "cot_summarize_chain = SimpleSequentialChain(chains=[cot_chain, summarize_chain])\n",
        "\n",
        "result = cot_summarize_chain.invoke(\n",
        "   {\"input\": \"저는 시장에 가서 사과 10개를 샀습니다. 이웃에게 2개, 수리공에게 2개를 주었습니다. \\\n",
        "   그런 다음에 사과 5개를 더 사서 1개를 먹었습니다. 남은 개수는 몇 개인가요?\"}\n",
        ")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "SfCKn90NWNdi",
        "outputId": "6483c308-c03a-48f5-cce7-4b608420b547"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "2 validation errors for SimpleSequentialChain\nchains -> 0\n  Can't instantiate abstract class Chain with abstract methods _call, input_keys, output_keys (type=type_error)\nchains -> 1\n  Can't instantiate abstract class Chain with abstract methods _call, input_keys, output_keys (type=type_error)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-8ccddb1c7d2f>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleSequentialChain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mcot_summarize_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleSequentialChain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchains\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcot_chain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarize_chain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m result = cot_summarize_chain.invoke(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mobject_setattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__dict__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for SimpleSequentialChain\nchains -> 0\n  Can't instantiate abstract class Chain with abstract methods _call, input_keys, output_keys (type=type_error)\nchains -> 1\n  Can't instantiate abstract class Chain with abstract methods _call, input_keys, output_keys (type=type_error)"
          ]
        }
      ]
    }
  ]
}