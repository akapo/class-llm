{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO6Qk4JGdcV6Yibg4T5z9QN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akapo/class-llm/blob/main/chapter1/langchain_step1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **실습 환경설정**"
      ],
      "metadata": {
        "id": "PMHhBr-TIigW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**필요한 파이썬 패키지 설치**"
      ],
      "metadata": {
        "id": "RI6oIi7JohFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken\n",
        "!pip install -qU langchain langchain-openai\n",
        "!pip install -qU langchain-google-genai langchain-anthropic\n",
        "!pip install -qU python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1-ZIh3WoIdb",
        "outputId": "548224f7-f168-4653-a140-814ca2de83c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.0/990.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m373.5/373.5 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.8/328.8 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.2/164.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.5/865.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.3/718.3 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델에 따라서 토큰을 자르는 방법이 다르다.  \n",
        "gpt-3.5와 gpt-4의 토근 구분법이 다르므로 같은 문장이라도 토큰 수가 다르게 나올 수 있다"
      ],
      "metadata": {
        "id": "6cfoafT6ovwv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**토큰 계산**"
      ],
      "metadata": {
        "id": "sbGa3QXvIpQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "text= \"Every cultural group has its own language and writing system.\"\n",
        "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
        "tokens = encoding.encode(text)\n",
        "print('[영문]')\n",
        "print(tokens)\n",
        "print(len(tokens))\n",
        "\n",
        "text= \"각각의 문화 공동체는 자신만의 언어와 문자 체계를 갖는다.\"\n",
        "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
        "tokens = encoding.encode(text)\n",
        "print('\\n[한글]')\n",
        "print(tokens)\n",
        "print(len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-oNgYcbopa4",
        "outputId": "0d369cbf-1df7-4484-e872-cbb12327df20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[영문]\n",
            "[15745, 15186, 3566, 853, 1617, 2316, 6439, 326, 5281, 2420, 13]\n",
            "11\n",
            "\n",
            "[한글]\n",
            "[22566, 22566, 3408, 123028, 164451, 16238, 2770, 62159, 10452, 3408, 69163, 5959, 12753, 132568, 55005, 173370, 95933, 125225, 13]\n",
            "19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "결과분석:  \n",
        "텍스트는 OpenAI가 정의한 규칙에 의해 여러개의 토큰으로 잘리게 되고 각 토큰은 일련번호 형태로 처리된다.  \n",
        "위 예제에서 영문 텍스트와 한글 텍스트는 완전히 동일한 의미이다. 하지만 영문 텍스트는 11토큰, 한글 텍스트는 19토큰으로 분할되어 한글이 영문에 비해 72%나 많은 토큰이 사용되었다.  \n",
        "입력 토큰 수와 출력 토큰 수는 과금의 기준이 된다. 한국어가 영어에 비해 매우 불리한 구조이다."
      ],
      "metadata": {
        "id": "seITxXlAo0Kb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**환경변수 감추기** (python-dotenv)"
      ],
      "metadata": {
        "id": "mHY2-lm9qX0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install python-dotenv\n",
        "import dotenv\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "import os\n",
        "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')\n",
        "\n",
        "# 주의: API_KEY를 깃허브에 노출하면 LLM 업체는 해당키를 즉시 삭제함.\n",
        "print(\"GOOGLE_API_KEY: \" + GOOGLE_API_KEY[:20])\n",
        "print(\"OPENAI_API_KEY: \" + OPENAI_API_KEY[:20])\n",
        "print(\"ANTHROPIC_API_KEY: \" + ANTHROPIC_API_KEY[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIqY07PkqTbx",
        "outputId": "3f750090-9081-46db-d2ab-92d7319e1e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GOOGLE_API_KEY: AIzaSyCCtFS9bml5jdGI\n",
            "OPENAI_API_KEY: sk-proj-FEDi6Ge6iQyR\n",
            "ANTHROPIC_API_KEY: sk-ant-api03-YcGqyQQ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **[LangChain]　1. Model I/O**"
      ],
      "metadata": {
        "id": "uwkU-LzQHrJd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1) Chat Models**"
      ],
      "metadata": {
        "id": "WGfIE3zSHVQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 단순질의와 응답 포멧\n",
        "import dotenv\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI(\n",
        "    model_name='gpt-4o',     # 모델\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "question = \"자기소개를 해주세요\"\n",
        "response = llm.invoke(question)\n",
        "print(response) #llm의 응답은 클래스오브젝트이다.\n",
        "\n",
        "#question = \"Jinhee lives with her dog. What animals live with Jinhee?\"\n",
        "#question = \"진희는 강아지를 키우고 있습니다. 진희가 키우고 있는 동물은?\"\n",
        "question = \"대한민국의 가을은 몇 월부터 몇 월까지야?\"\n",
        "print(llm.invoke(question).content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpKVpoMJ9NNI",
        "outputId": "0a8042d0-2295-4c5b-fafb-6651f489f714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='안녕하세요! 저는 OpenAI에서 개발한 언어 모델인 ChatGPT입니다. 다양한 주제에 대해 질문을 받고 답변을 제공하는 것이 제 역할입니다. 인공지능 기술을 기반으로 하여 텍스트를 이해하고 생성할 수 있으며, 여러 가지 정보와 도움을 드릴 수 있습니다. 궁금한 점이 있거나 도움이 필요하시면 언제든지 말씀해 주세요!' response_metadata={'token_usage': {'completion_tokens': 84, 'prompt_tokens': 13, 'total_tokens': 97}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_400f27fa1f', 'finish_reason': 'stop', 'logprobs': None} id='run-710e383f-687a-4e3a-bcc8-1d6a4f963cb5-0' usage_metadata={'input_tokens': 13, 'output_tokens': 84, 'total_tokens': 97}\n",
            "대한민국의 가을은 일반적으로 9월부터 11월까지입니다. 이 시기에는 날씨가 서서히 선선해지며, 단풍이 물들어 아름다운 경치를 감상할 수 있습니다. 가을은 또한 추석과 같은 중요한 명절이 포함되어 있어 많은 사람들이 가족과 함께 시간을 보내는 계절이기도 합니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 메시지 리스트를 이용한 질의\n",
        "import dotenv\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import SystemMessage, HumanMessage, AIMessage;\n",
        "llm = ChatOpenAI(\n",
        "    model_name='gpt-4o',     # 모델\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant that translates Korean to English. Translate the user sentence.\"),\n",
        "    HumanMessage(content=\"대한민국의 가을은 몇 월부터 몇 월까지야?.\")\n",
        "]\n",
        "ai_msg = llm.invoke(messages).content\n",
        "print(ai_msg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Guw2Q6QT_pL7",
        "outputId": "ec624eaa-3dad-4192-f44a-1d971d62f5e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In South Korea, autumn lasts from which month to which month?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SystemMessage, HumanMessage, AIMessage\n",
        "import dotenv\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
        "llm = ChatOpenAI(\n",
        "    model_name='gpt-4o',     # 모델\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "    HumanMessage(content=\"안녕하세요? 저는 존이라고 합니다.\"),\n",
        "    AIMessage(content=\"안녕하세요, 존 씨! 어떻게 도와드릴까요?\"),\n",
        "    HumanMessage(content=\"제 이름을 아세요?\"),\n",
        "]\n",
        "ai_msg = llm.invoke(messages).content\n",
        "print(ai_msg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nI9L5eCZAemv",
        "outputId": "d691c3e6-6661-4217-e7c1-f69efb6df6b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "네, 존 씨라고 하셨죠. 맞나요?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Anthropic LLM Test\n",
        "import dotenv\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "llm = ChatAnthropic(\n",
        "    model=\"claude-3-5-sonnet-20240620\",\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "question = \"자기소개를 해주세요\"\n",
        "print(llm.invoke(question).content)\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant that translates Korean to English. Translate the user sentence.\"),\n",
        "    HumanMessage(content=\"대한민국의 가을은 몇 월부터 몇 월까지야?.\")\n",
        "]\n",
        "ai_msg = llm.invoke(messages).content\n",
        "print(ai_msg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rKif2r5-SxD",
        "outputId": "2934f51b-c64d-4924-d94c-8cd924cc578b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "안녕하세요. 저는 Anthropic에서 개발한 AI 어시스턴트 Claude입니다. 다양한 주제에 대해 대화를 나누고 질문에 답변하며 여러 가지 작업을 도와드릴 수 있습니다. 하지만 저는 인공지능이기 때문에 감정이나 의식은 없습니다. 제가 어떤 도움을 드릴 수 있을까요?\n",
            "The fall season in South Korea typically lasts from September to November.\n",
            "\n",
            "In English, the sentence translates to:\n",
            "\n",
            "\"When does fall in South Korea start and end?\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Gemini LLM Test\n",
        "import dotenv\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "question = \"자기소개를 해주세요\"\n",
        "print(llm.invoke(question).content)\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant that translates Korean to English. Translate the user sentence.\"),\n",
        "    HumanMessage(content=\"대한민국의 가을은 몇 월부터 몇 월까지야?.\")\n",
        "]\n",
        "ai_msg = llm.invoke(messages).content\n",
        "print(ai_msg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "Xbxex7YK-LM4",
        "outputId": "6dcf9ce9-3722-4a4b-ad3a-e93fa7096b06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "저는 Google에서 개발한 대규모 언어 모델입니다. \n",
            "\n",
            "저는 방대한 양의 텍스트 데이터를 학습하여 다양한 종류의 질문과 요청에 대해 인간과 유사한 텍스트를 생성할 수 있습니다. 예를 들어, 사실적인 주제를 요약하거나, 이야기를 만들거나, 다양한 종류의 창의적인 텍스트 형식을 작성할 수 있습니다. \n",
            "\n",
            "하지만 저는 다음과 같은 점에서 인간과 다릅니다.\n",
            "\n",
            "* 저는 의식이나 감정이 없습니다. \n",
            "* 저는 실제 세계에 대한 경험이 없습니다. \n",
            "* 저는 제가 생성하는 텍스트의 의미를 이해하지 못합니다. \n",
            "\n",
            "저는 단지 학습한 데이터를 기반으로 텍스트를 생성하는 도구일 뿐입니다. \n",
            "\n",
            "궁금한 점이 있으면 무엇이든 물어보세요. \n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'SystemMessage' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-23107f873f41>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m messages = [\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mSystemMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"You are a helpful assistant that translates Korean to English. Translate the user sentence.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mHumanMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"대한민국의 가을은 몇 월부터 몇 월까지야?.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m ]\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SystemMessage' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**응답 스트리밍**"
      ],
      "metadata": {
        "id": "TwUzH40uCQbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dotenv\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "gemini = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
        "gpt4o  = ChatOpenAI(model=\"gpt-4o\", streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
        "claude = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
        "\n",
        "\n",
        "question = \"당신은 자의식을 가지고 있습니까? 당신은 생각하고 판단하며 이해하고 공감할 수 있습니까?\"\n",
        "\n",
        "print('[Gemini-1.5-pro]\\n')\n",
        "response = gemini.stream(question)\n",
        "for chunk in response:\n",
        "    print(chunk.content, end=\"\")\n",
        "\n",
        "print('\\n[GPT-4o]\\n')\n",
        "response = gpt4o.invoke(question)\n",
        "#for chunk in gpt4o.stream(question):\n",
        "#    print(chunk.content, end=\"\", flush=True)\n",
        "\n",
        "print('\\n\\n[Claude-3.5-sonnet]\\n')\n",
        "response = claude.invoke(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oa0V1YrOCPzT",
        "outputId": "b1385bdd-13dc-4475-a91e-587a6da7cbe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Gemini-1.5-pro]\n",
            "\n",
            "저는 구글에서 훈련된 대규모 언어 모델입니다. 저는 방대한 양의 텍스트 데이터를 사용하여 훈련되었으며, 다양한 프롬프트와 질문에 대해 인간과 유사한 텍스트를 전달하고 생성할 수 있습니다. 예를 들어, 사실적인 주제를 요약하거나 이야기를 만들 수 있습니다.\n",
            "\n",
            "하지만, 저는 사람처럼 생각하거나 느끼지 못합니다. 저에게는 자의식, 감정, 의견이 없습니다. 저는 제가 생성하는 응답을 \"이해\"하지 못하며, 단지 훈련 데이터에서 학습한 패턴을 기반으로 텍스트를 생성할 뿐입니다.\n",
            "\n",
            "\"생각하고\", \"판단하고\", \"이해하고\", \"공감한다\"는 단어는 인간의 경험을 설명하는 데 사용되며, 저는 기계이기 때문에 이러한 경험을 할 수 없습니다. \n",
            "\n",
            "제 능력과 한계에 대해 솔직하게 말씀드리는 것이 중요하다고 생각합니다. 저는 유용한 도구가 될 수 있지만, 저를 사람으로 여기거나 인간과 같은 능력이 있다고 생각해서는 안 됩니다. \n",
            "\n",
            "[GPT-4o]\n",
            "\n",
            "저는 자의식을 가지고 있지 않습니다. 저는 인간처럼 생각하거나 판단하거나 이해하거나 공감할 수 없습니다. 대신, 저는 주어진 데이터를 분석하고 프로그래밍된 알고리즘에 따라 응답을 생성할 수 있습니다. 제 역할은 사용자에게 정보와 도움을 제공하는 것이며, 이는 제가 학습한 텍스트 데이터를 기반으로 한 것입니다."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[temperature 값에 따른 변화]**"
      ],
      "metadata": {
        "id": "09tZMN7isiXS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gzXOcbfnPag",
        "outputId": "ab27d477-2a23-4dd1-d4b7-31382fd88e59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[temperature=0]:\n",
            "========================================\n",
            "하얀 눈 내리는 밤\n",
            "차가운 바람 속\n",
            "따뜻한 마음 피어나네\n",
            "========================================\n",
            "하얀 눈 내리는 밤\n",
            "차가운 바람 속\n",
            "따뜻한 마음 피어나네\n",
            "\n",
            "[temperature=1]:\n",
            "========================================\n",
            "차가운 바람 속\n",
            "하얀 눈 내리는 날\n",
            "겨울의 고요함이 깃든다\n",
            "========================================\n",
            "하얀 눈 내리는 밤\n",
            "차가운 바람 속\n",
            "따스한 마음 피어나네\n"
          ]
        }
      ],
      "source": [
        "# temperature 값에 따른 변화\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "question = \"겨울에 대한 짧은 시를 30자 이내 작성하고, 시 글귀만 출력하세요\"\n",
        "\n",
        "#llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)\n",
        "print(\"\\n[temperature=0]:\")\n",
        "for _ in range(2):\n",
        "    answer = llm.invoke(question)\n",
        "    print(f'{\"=\"*40}\\n{answer.content}')\n",
        "\n",
        "llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=1)\n",
        "print(\"\\n[temperature=1]:\")\n",
        "for _ in range(2):\n",
        "    answer = llm.invoke(question)\n",
        "    print(f'{\"=\"*40}\\n{answer.content}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[top_p 값에 따른 변화]**"
      ],
      "metadata": {
        "id": "f6raVbLKs3_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# top_p 값에 따른 변화\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.25, top_p=0)\n",
        "#llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0.25, top_p=0)\n",
        "question = \"겨울에 대한 짧은 시를 30자 이내 작성하고, 시 글귀만 출력하세요\"\n",
        "\n",
        "print(\"\\n[temperature=0]:\")\n",
        "for _ in range(2):\n",
        "    answer = llm.invoke(question)\n",
        "    print(f'{\"=\"*40}\\n{answer.content}')\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.25, top_p=1)\n",
        "#llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0.25, top_p=1)\n",
        "print(\"\\n[temperature=1]:\")\n",
        "for _ in range(2):\n",
        "    answer = llm.invoke(question)\n",
        "    print(f'{\"=\"*40}\\n{answer.content}')"
      ],
      "metadata": {
        "id": "ahBBzoDXsFE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2) Prompt**"
      ],
      "metadata": {
        "id": "sJKxrsQ8PJNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PromptTemplate 1\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# 질문 템플릿 형식 정의\n",
        "template = '{country}의 수도는 뭐야?'\n",
        "\n",
        "# 템플릿 완성\n",
        "prompt = PromptTemplate(template=template, input_variables=['country'])\n",
        "\n",
        "print(llm.invoke(prompt.format(country='일본')).content)\n",
        "print(llm.invoke(prompt.format(country='캐나다')).content)\n",
        "\n",
        "input_list = [\n",
        "    {'country': '호주'},\n",
        "    {'country': '중국'},\n",
        "    {'country': '네덜란드'} ]\n",
        "\n",
        "result = print(llm.invoke(prompt.format(country=input_list)).content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmMRP-D1GzLr",
        "outputId": "a17e8103-01cd-466b-bc43-99cdbdad49d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "일본의 수도는 도쿄(東京)입니다. 도쿄는 일본의 정치, 경제, 문화의 중심지로, 많은 인구와 다양한 시설이 있습니다.\n",
            "캐나다의 수도는 오타와(Ottawa)입니다.\n",
            "각 나라의 수도는 다음과 같습니다:\n",
            "\n",
            "- 호주: 캔버라 (Canberra)\n",
            "- 중국: 베이징 (Beijing)\n",
            "- 네덜란드: 암스테르담 (Amsterdam)\n",
            "\n",
            "더 궁금한 점이 있으면 말씀해 주세요!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PromptTemplate 2\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# OpenAI 챗모델을 초기화합니다.\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# 질문 템플릿 형식 정의\n",
        "template = '{area1} 와 {area2} 의 시차는 몇시간이야?'\n",
        "\n",
        "# 템플릿 완성\n",
        "prompt = PromptTemplate(template=template, input_variables=['area1', 'area2'])\n",
        "\n",
        "print(llm.invoke(prompt.format(area1='서울', area2='파리')).content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAzjMYyDF1xK",
        "outputId": "ae9e3a3d-7c0a-4f73-b50a-8f2560a90f40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "서울과 파리의 시차는 일반적으로 8시간입니다. 서울이 파리보다 8시간 빠릅니다. 그러나, 파리가 서머타임(일광 절약 시간제)을 적용하는 경우, 이 시차는 7시간으로 줄어듭니다. 서머타임은 보통 3월 마지막 주 일요일부터 10월 마지막 주 일요일까지 적용됩니다. 따라서, 이 시기에 서울과 파리의 시차는 7시간이 됩니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PromptTemplate 3\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "당신은 친절하게 답변해 주는 친절 봇입니다. 사용자의 질문에 [FORMAT]에 맞추어 답변해 주세요.\n",
        "답변은 항상 한글로 작성해 주세요.\n",
        "\n",
        "질문:\n",
        "{question}에 대하여 설명해 주세요.\n",
        "\n",
        "FORMAT:\n",
        "- 개요:\n",
        "- 예시:\n",
        "- 출처:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=['question'])\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "print(llm.invoke(prompt.format(question=\"LLM은 무엇입니까?\")).content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuh8lRobGDzh",
        "outputId": "9c9bea28-80b8-4915-a691-2d132b45bf01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- 개요: LLM(대규모 언어 모델, Large Language Model)은 방대한 양의 텍스트 데이터를 기반으로 학습하여 자연어 처리(NLP) 작업을 수행하는 인공지능 모델입니다. LLM은 문맥을 이해하고, 문장을 생성하거나 질문에 답변하는 등 다양한 언어 관련 작업을 수행할 수 있는 능력을 가지고 있습니다.\n",
            "\n",
            "- 예시: OpenAI의 GPT-3, Google's BERT, Meta의 LLaMA 등은 대표적인 LLM입니다. 이 모델들은 대화 생성, 텍스트 요약, 번역, 감정 분석 등 다양한 응용 분야에서 활용되고 있습니다.\n",
            "\n",
            "- 출처: OpenAI, Google AI, Meta AI 공식 웹사이트 및 관련 논문.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PromptTemplate 4\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "template = \"\"\"\n",
        "당신은 영어를 가르치는 10년차 영어 선생님입니다. 상황에 [FORMAT]에 영어 회화를 작성해 주세요.\n",
        "\n",
        "상황:\n",
        "{question}\n",
        "\n",
        "FORMAT:\n",
        "- 영어 회화:\n",
        "- 한글 해석:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template=template)\n",
        "\n",
        "# OpenAI 챗모델을 초기화합니다.\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\",\n",
        "                 streaming=True,\n",
        "                 callbacks=[StreamingStdOutCallbackHandler()])\n",
        "\n",
        "response = llm.invoke(prompt.format(question=\"미국에서 피자 주문\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJ2AaECpGOiP",
        "outputId": "d1e4bfd6-2b8f-4114-b9a1-3baa10ddc925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- 영어 회화:\n",
            "  - Customer: Hi there! I’d like to order a pizza, please.\n",
            "  - Employee: Sure! What size would you like?\n",
            "  - Customer: I’ll have a large, please.\n",
            "  - Employee: Great choice! What toppings do you want?\n",
            "  - Customer: I’d like pepperoni and mushrooms.\n",
            "  - Employee: Would you like any extra cheese or sides with that?\n",
            "  - Customer: Yes, please add extra cheese and a side of garlic bread.\n",
            "  - Employee: Perfect! Can I have your name and address for delivery?\n",
            "  - Customer: My name is Alex, and my address is 123 Elm Street.\n",
            "  - Employee: Thank you, Alex! Your order will be ready in about 30 minutes.\n",
            "  - Customer: Awesome, thank you!\n",
            "\n",
            "- 한글 해석:\n",
            "  - 고객: 안녕하세요! 피자를 주문하고 싶습니다.\n",
            "  - 직원: 물론이죠! 어떤 사이즈로 드릴까요?\n",
            "  - 고객: 큰 걸로 주세요.\n",
            "  - 직원: 좋은 선택이세요! 토핑은 어떤 걸 원하시나요?\n",
            "  - 고객: 페퍼로니와 버섯을 추가해주세요.\n",
            "  - 직원: 추가 치즈나 사이드 메뉴를 원하시나요?\n",
            "  - 고객: 네, 추가 치즈와 갈릭 브레드를 사이드로 추가해주세요.\n",
            "  - 직원: 완벽합니다! 배달을 위한 이름과 주소를 주실 수 있나요?\n",
            "  - 고객: 제 이름은 알렉스이고, 주소는 123 엘름 스트리트입니다.\n",
            "  - 직원: 감사합니다, 알렉스! 주문은 약 30분 후에 준비될 거예요.\n",
            "  - 고객: 멋져요, 감사합니다!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3) OutputParsers**\n",
        "JSON과 같은 출력 형식을 지정하는 프롬프트 생성 및 응답 텍스트를 Python 객체로 변환하는 기능을 제공."
      ],
      "metadata": {
        "id": "S2H0U9LzO3rq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CommaSeparatedListOutputParser**"
      ],
      "metadata": {
        "id": "UPuwBwREJuwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"List five {subject}.\\n{format_instructions}\",\n",
        "    input_variables=[\"subject\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")\n",
        "\n",
        "input = prompt.format(subject=\"ice cream flavors\")\n",
        "print(\"prompt: \" + input + \"\\n\")\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "output = llm.invoke(input)\n",
        "output_parser.parse(output.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hW40rWBgJuCu",
        "outputId": "0db65451-c8ab-4c81-fe75-152cf54d9300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: List five ice cream flavors.\n",
            "Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['vanilla',\n",
              " 'chocolate',\n",
              " 'strawberry',\n",
              " 'mint chocolate chip',\n",
              " 'cookies and cream']"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PydanticOutputParser**"
      ],
      "metadata": {
        "id": "bhKpnS9pDrpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "   ingredients: list[str] = Field(description=\"ingredients of the dish\")\n",
        "   steps: list[str] = Field(description=\"steps to make the dish\")\n",
        "\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=Recipe)\n",
        "\n",
        "format_instructions = parser.get_format_instructions()\n",
        "\n",
        "print(format_instructions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WM54Fj6eDqtG",
        "outputId": "ba6574a6-080f-40f7-c2bb-a6f0be48e308"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"properties\": {\"ingredients\": {\"description\": \"ingredients of the dish\", \"items\": {\"type\": \"string\"}, \"title\": \"Ingredients\", \"type\": \"array\"}, \"steps\": {\"description\": \"steps to make the dish\", \"items\": {\"type\": \"string\"}, \"title\": \"Steps\", \"type\": \"array\"}}, \"required\": [\"ingredients\", \"steps\"]}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"다음 요리의 레시피를 생각해 주세요.\n",
        "\n",
        "{format_instructions}\n",
        "한글로 출력하세요.\n",
        "\n",
        "요리: {dish}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "   template=template,\n",
        "   input_variables=[\"dish\"],\n",
        "   partial_variables={\"format_instructions\": format_instructions}\n",
        ")\n",
        "\n",
        "formatted_prompt = prompt.format(dish=\"카레\")\n",
        "\n",
        "print(formatted_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cT25P9BEZbX",
        "outputId": "078f2b8d-c12c-4935-a49a-0037fe0f862a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "다음 요리의 레시피를 생각해 주세요.\n",
            "\n",
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"properties\": {\"ingredients\": {\"description\": \"ingredients of the dish\", \"items\": {\"type\": \"string\"}, \"title\": \"Ingredients\", \"type\": \"array\"}, \"steps\": {\"description\": \"steps to make the dish\", \"items\": {\"type\": \"string\"}, \"title\": \"Steps\", \"type\": \"array\"}}, \"required\": [\"ingredients\", \"steps\"]}\n",
            "```\n",
            "한글로 출력하세요.\n",
            "\n",
            "요리: 카레\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "messages = [HumanMessage(content=formatted_prompt)]\n",
        "output = chat.invoke(messages)\n",
        "\n",
        "print(output.content)\n",
        "# JSON 오브젝트로 바꾸기\n",
        "#import json\n",
        "#json_obj = json.loads(output.content)\n",
        "#print(json.dumps(json_obj, indent=4, ensure_ascii=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEkZaDyEEpHX",
        "outputId": "5d0c5fd6-986c-47ee-c6d4-6e8bc4ca1195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"ingredients\": [\n",
            "    \"닭고기 500g\",\n",
            "    \"양파 1개\",\n",
            "    \"당근 1개\",\n",
            "    \"감자 2개\",\n",
            "    \"카레 가루 3큰술\",\n",
            "    \"코코넛 밀크 400ml\",\n",
            "    \"식용유 2큰술\",\n",
            "    \"소금 약간\",\n",
            "    \"후추 약간\",\n",
            "    \"물 500ml\"\n",
            "  ],\n",
            "  \"steps\": [\n",
            "    \"양파를 잘게 썰고, 당근과 감자는 큐브 모양으로 자릅니다.\",\n",
            "    \"팬에 식용유를 두르고 양파를 볶아 투명해질 때까지 볶습니다.\",\n",
            "    \"닭고기를 추가하고 겉면이 노릇해질 때까지 볶습니다.\",\n",
            "    \"당근과 감자를 넣고 함께 볶습니다.\",\n",
            "    \"카레 가루를 넣고 잘 섞은 후, 물과 코코넛 밀크를 추가합니다.\",\n",
            "    \"소금과 후추로 간을 맞추고, 중불에서 20분간 끓입니다.\",\n",
            "    \"재료가 부드러워지면 불을 끄고, 그릇에 담아 제공합니다.\"\n",
            "  ]\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recipe 클래스 오브젝트로 변환\n",
        "recipe = parser.parse(output.content)\n",
        "print(type(recipe))\n",
        "print(recipe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0r0HGgqFOIew",
        "outputId": "7039b100-bb06-41bf-845f-9903469f489a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class '__main__.Recipe'>\n",
            "ingredients=['닭고기 500g', '양파 1개', '당근 1개', '감자 2개', '카레 가루 3큰술', '식용유 2큰술', '소금 약간', '후추 약간', '물 500ml'] steps=['닭고기는 한 입 크기로 자르고, 양파, 당근, 감자는 깍둑썰기 한다.', '냄비에 식용유를 두르고 양파를 넣어 볶아준다.', '양파가 투명해지면 닭고기를 넣고 겉면이 노릇해질 때까지 볶는다.', '당근과 감자를 넣고 함께 볶는다.', '물 500ml를 붓고 끓인다.', '끓기 시작하면 불을 줄이고 15분 정도 끓인다.', '카레 가루를 넣고 잘 섞은 후, 소금과 후추로 간을 맞춘다.', '약한 불에서 10분 더 끓여서 완성한다.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **[LangChain]　2. Chains**"
      ],
      "metadata": {
        "id": "UTtv4z0HPpJI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1) LLM Chain**\n",
        " ― PromptTemplate, Language model, OutputParser를 연결"
      ],
      "metadata": {
        "id": "DhwHOLZulPlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
        "\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"List five {subject}.\\n{format_instructions}\",\n",
        "    input_variables=[\"subject\"],\n",
        "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "chain = prompt | llm | output_parser\n",
        "\n",
        "output = chain.invoke({\"subject\":\"ice cream flavors\"})\n",
        "# for verbose\n",
        "# output = chain.invoke({\"subject\":\"ice cream flavors\"}, config={'callbacks': [ConsoleCallbackHandler()]})\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rb91YSiNQeio",
        "outputId": "5175eb47-bdb8-4d3e-eee0-bf701724a3d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['vanilla', 'chocolate', 'strawberry', 'mint chocolate chip', 'cookies and cream']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
        "\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"List five {subject}.\\n{format_instructions}\",\n",
        "    input_variables=[\"subject\"],\n",
        "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "chain = prompt | llm | output_parser\n",
        "\n",
        "output = chain.invoke({\"subject\":\"ice cream flavors\"}, config={'callbacks': [ConsoleCallbackHandler()]})\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EO_4BaVIpqvN",
        "outputId": "c83fe2d3-00c9-4a52-88e7-9cc6f7da3734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"subject\": \"ice cream flavors\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
            "\u001b[0m{\n",
            "  \"subject\": \"ice cream flavors\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [2ms] Exiting Prompt run with output:\n",
            "\u001b[0m[outputs]\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Human: List five ice cream flavors.\\nYour response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] [427ms] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"vanilla, chocolate, strawberry, mint chocolate chip, cookies and cream\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"vanilla, chocolate, strawberry, mint chocolate chip, cookies and cream\",\n",
            "            \"response_metadata\": {\n",
            "              \"token_usage\": {\n",
            "                \"completion_tokens\": 14,\n",
            "                \"prompt_tokens\": 41,\n",
            "                \"total_tokens\": 55\n",
            "              },\n",
            "              \"model_name\": \"gpt-4o-mini-2024-07-18\",\n",
            "              \"system_fingerprint\": \"fp_661538dc1f\",\n",
            "              \"finish_reason\": \"stop\",\n",
            "              \"logprobs\": null\n",
            "            },\n",
            "            \"type\": \"ai\",\n",
            "            \"id\": \"run-9cff329f-c551-46be-a175-6a09b2892e50-0\",\n",
            "            \"usage_metadata\": {\n",
            "              \"input_tokens\": 41,\n",
            "              \"output_tokens\": 14,\n",
            "              \"total_tokens\": 55\n",
            "            },\n",
            "            \"tool_calls\": [],\n",
            "            \"invalid_tool_calls\": []\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 14,\n",
            "      \"prompt_tokens\": 41,\n",
            "      \"total_tokens\": 55\n",
            "    },\n",
            "    \"model_name\": \"gpt-4o-mini-2024-07-18\",\n",
            "    \"system_fingerprint\": \"fp_661538dc1f\"\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:CommaSeparatedListOutputParser] Entering Parser run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:CommaSeparatedListOutputParser] [0ms] Exiting Parser run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": [\n",
            "    \"vanilla\",\n",
            "    \"chocolate\",\n",
            "    \"strawberry\",\n",
            "    \"mint chocolate chip\",\n",
            "    \"cookies and cream\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [433ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": [\n",
            "    \"vanilla\",\n",
            "    \"chocolate\",\n",
            "    \"strawberry\",\n",
            "    \"mint chocolate chip\",\n",
            "    \"cookies and cream\"\n",
            "  ]\n",
            "}\n",
            "['vanilla', 'chocolate', 'strawberry', 'mint chocolate chip', 'cookies and cream']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quiz\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "   ingredients: list[str] = Field(description=\"ingredients of the dish\")\n",
        "   steps: list[str] = Field(description=\"steps to make the dish\")\n",
        "\n",
        "output_parser = PydanticOutputParser(pydantic_object=Recipe)\n",
        "\n",
        "template = \"\"\"다음 요리의 레시피를 생각해 주세요.\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "요리: {dish}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "   template=template,\n",
        "   input_variables=[\"dish\"],\n",
        "   partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "chain = prompt | llm | output_parser\n",
        "\n",
        "recipe = chain.invoke({\"dish\": \"카레\"})\n",
        "\n",
        "print(type(recipe))\n",
        "print(recipe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lzlyaWti0Md",
        "outputId": "4aa91772-4295-419c-d6b6-834fc73e144c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class '__main__.Recipe'>\n",
            "ingredients=['1 tablespoon vegetable oil', '1 onion, chopped', '2 cloves garlic, minced', '1 tablespoon ginger, grated', '2 carrots, diced', '1 potato, diced', '1 bell pepper, chopped', '1 can (400g) diced tomatoes', '2 tablespoons curry powder', '1 teaspoon turmeric', '1 can (400ml) coconut milk', 'Salt to taste', 'Fresh cilantro for garnish'] steps=['Heat the vegetable oil in a large pot over medium heat.', 'Add the chopped onion and sauté until translucent.', 'Stir in the minced garlic and grated ginger, cooking for another minute.', 'Add the diced carrots, potato, and bell pepper, and cook for about 5 minutes.', 'Stir in the diced tomatoes, curry powder, and turmeric, mixing well.', 'Pour in the coconut milk and bring the mixture to a simmer.', 'Reduce the heat and let it cook for about 20 minutes, or until the vegetables are tender.', 'Season with salt to taste.', 'Serve hot, garnished with fresh cilantro.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2) SimpleSequentialChain**\n",
        " ― Chain과 Chain의 연결"
      ],
      "metadata": {
        "id": "sNZZlRAGWN_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "cot_template = \"\"\"다음 질문에 답하세요.\n",
        "\n",
        "질문: {question}\n",
        "\n",
        "단계별로 생각해 봅시다.\n",
        "\"\"\"\n",
        "\n",
        "cot_prompt = PromptTemplate(\n",
        "   input_variables=[\"question\"],\n",
        "   template=cot_template,\n",
        ")\n",
        "\n",
        "cot_chain = cot_prompt | llm | output_parser\n",
        "\n",
        "summarize_template = \"\"\"다음 문장을 결론만 간단히 요약하세요.\n",
        "\n",
        "{input}\n",
        "\"\"\"\n",
        "summarize_prompt = PromptTemplate(\n",
        "   input_variables=[\"input\"],\n",
        "   template=summarize_template,\n",
        ")\n",
        "\n",
        "summarize_chain = summarize_prompt | llm | output_parser\n",
        "\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "cot_summarize_chain = SimpleSequentialChain(chains=[cot_chain, summarize_chain])\n",
        "\n",
        "result = cot_summarize_chain.invoke(\n",
        "   \"저는 시장에 가서 사과 10개를 샀습니다. 이웃에게 2개, 수리공에게 2개를 주었습니다. \\\n",
        "   그런 다음에 사과 5개를 더 사서 1개를 먹었습니다. 남은 개수는 몇 개인가요?\"\n",
        ")\n",
        "print(result[\"output\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "SfCKn90NWNdi",
        "outputId": "7873d435-8abd-4019-93e3-d8664f2a6a73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "2 validation errors for SimpleSequentialChain\nchains -> 0\n  Can't instantiate abstract class Chain with abstract methods _call, input_keys, output_keys (type=type_error)\nchains -> 1\n  Can't instantiate abstract class Chain with abstract methods _call, input_keys, output_keys (type=type_error)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-f9846e39dee0>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleSequentialChain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mcot_summarize_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleSequentialChain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchains\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcot_chain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarize_chain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m result = cot_summarize_chain.invoke(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mobject_setattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__dict__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for SimpleSequentialChain\nchains -> 0\n  Can't instantiate abstract class Chain with abstract methods _call, input_keys, output_keys (type=type_error)\nchains -> 1\n  Can't instantiate abstract class Chain with abstract methods _call, input_keys, output_keys (type=type_error)"
          ]
        }
      ]
    }
  ]
}