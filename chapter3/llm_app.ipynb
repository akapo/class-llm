{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMy6r89vY8FMLhCxKcYGT6F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akapo/class-llm/blob/main/chapter3/llm_app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 실습 환경 설정"
      ],
      "metadata": {
        "id": "1Qdc2CmbE69p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU openai\n",
        "!pip install -qU langchain langchain-openai langchain-community langchainhub langchain-experimental langgraph\n",
        "!pip install -qU huggingface_hub langchain-google-genai langchain-anthropic\n",
        "!pip install -Uq sentence-transformers\n",
        "!pip install -qU python-dotenv\n",
        "!pip install -qU pymupdf pypdf unstructured markdown\n",
        "!pip install -qU numexpr\n",
        "!pip install -qU faiss-cpu\n",
        "!pip install -qU chromadb\n",
        "!pip install -qU bs4\n",
        "!pip install -qU google-search-results duckduckgo-search\n",
        "!pip install -qU streamlit\n",
        "!pip install -qU streamlit-chat"
      ],
      "metadata": {
        "id": "lvBig087ErWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "langchain.debug = False\n",
        "langchain.verbose = False\n",
        "\n",
        "# LangSmith 를 이용한 모니터링\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"chain-monitor\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"]=\"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\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\""
      ],
      "metadata": {
        "id": "NCEJXTmhBeMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LLM활용 인공지능 앱 만들기**"
      ],
      "metadata": {
        "id": "gmXXlTEmhzF2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Web index 파일**  \n",
        "- 저장위치: **home.py**"
      ],
      "metadata": {
        "id": "dJCqm3nqItdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"Langchain Streamlit App Examples\",\n",
        "    page_icon='💬',\n",
        "    layout='wide'\n",
        ")\n",
        "\n",
        "st.header(\"Chatbot Implementations with Langchain + Streamlit\")\n",
        "st.write(\"\"\"\n",
        "[![view source code ](https://img.shields.io/badge/GitHub%20Repository-gray?logo=github)](https://github.com/akapo/llm-app)\n",
        "\"\"\")\n",
        "st.write(\"\"\"\n",
        "Langchain은 LLM(언어 모델)을 사용하여 애플리케이션 개발을 간소화하도록 설계된 강력한 프레임워크입니다. 다양한 구성 요소의 포괄적인 통합을 제공하여 강력한 응용 프로그램을 만들기 위해 조립 프로세스를 단순화합니다.\n",
        "\n",
        "Langchain의 힘을 활용하면 챗봇 생성이 쉬워집니다. 다음은 다양한 사용 사례에 맞는 챗봇 구현의 몇 가지 예입니다.\n",
        "\n",
        "- **💬translato**: 번역 서비스 앱(다양한 언어 지원).\n",
        "- **online_chatbot**: 인터넷에 접속하여 정보를 검색하는 챗봇 구현.\n",
        "- **💽memorye chatbot**: 컨텍스트를 유지하며 기억력을 가지는 챗봇 구현.\n",
        "- **📄rag_chatbot**: 입력해준 문서를 기반으로 답변을 생성하는 챗봇 구현.\n",
        "- **⭐coteacher**: 프로그래밍 인공지능 보조교사 구현.\n",
        "- **🎓Knowlegebase**: 인공지능 보조교사에게 지식 주입.\n",
        "\n",
        "각 챗봇의 샘플 사용법을 살펴보려면 해당 챗봇 섹션으로 이동하세요.\"\"\")"
      ],
      "metadata": {
        "id": "kJ4c6vOJCuVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1) 번역 서비스 앱**"
      ],
      "metadata": {
        "id": "Ubd7dP3mOx9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**번역 서비스 앱**  \n",
        "- 저장위치: **pages/1_💬translator.py**"
      ],
      "metadata": {
        "id": "wDiIvahuJYXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import streamlit as st\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "langs = [\"English\", \"Japanese\", \"Chinese\",\n",
        "         \"Korean\",  \"Italian\", \"French\", \"Spanish\",\n",
        "         \"Russian\", \"Vietnamise\"]  #번역 언어를 나열\n",
        "\n",
        "st.set_page_config(page_title=\"언어 번역 서비스\", page_icon=\"💬\", layout='wide')\n",
        "st.header('언어 번역 서비스')\n",
        "\n",
        "#웹페이지 왼쪽에 언어를 선택할 수 있는 라디오 버튼\n",
        "with st.sidebar:\n",
        "     language = st.radio('번역을 원하는 언어(출력)를 선택해주세요.:', langs)\n",
        "\n",
        "# text_area에 입력된 사용자의 텍스트\n",
        "prompt = st.text_area('번역을 원하는 텍스트를 입력하세요(언어 자동감지)')\n",
        "\n",
        "trans_template = PromptTemplate(\n",
        "    input_variables=['trans'],\n",
        "    # '당신의 일은 이 텍스트를 ___어로 번역하는 것입니다.\\n TEXT: {trans}'\n",
        "    template='Your task is to translate this text to ' + language +\n",
        "    'Print only the translation results.\\nTEXT: {trans}'\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.0)\n",
        "\n",
        "trans_chain = LLMChain(\n",
        "    llm=llm, prompt=trans_template, verbose=True, output_key='translate')\n",
        "\n",
        "# 프롬프트(prompt)가 있으면 이를 처리하고 화면에 응답을 작성\n",
        "if st.button(\"번역\"):\n",
        "    if prompt:\n",
        "        response = trans_chain({'trans': prompt})\n",
        "        st.info(response['translate'])"
      ],
      "metadata": {
        "id": "OqISQByFMPkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2) 기본 챗봇**"
      ],
      "metadata": {
        "id": "lwDe89dOhpnC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXiqck7gA6Nb"
      },
      "outputs": [],
      "source": [
        "# 챗봇 기본\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import streamlit as st\n",
        "st.title(\"기본 챗봇 \")\n",
        "\n",
        "prompt = st.chat_input(\"하고 싶은 말\")\n",
        "\n",
        "if prompt:  # 입력된 문자열이 있는 경우(None도 아니고 빈 문자열도 아닌 경우)\n",
        "    with st.chat_message(\"user\"): # 사용자 아이콘 사용\n",
        "        st.markdown(prompt) # 입력된 받은 내용을 마크다운으로 해석하여 표시\n",
        "\n",
        "    with st.chat_message(\"assistant\"): # AI 아이콘 사용\n",
        "        response = \"대답함\"   # 응답 내용을 \"대답함\" 이라는 문자열로 설정\n",
        "        st.markdown(response) # 응답 내용을 마크다운으로 해석하여 표시\n",
        "\n",
        "# 문제점: 이전 대화 내용 사라짐. 항상 응답이 \"대답함\"임"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 대화내용 보존 업그레이드\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import streamlit as st\n",
        "# StreamlitChatMessageHistory 추가\n",
        "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
        "\n",
        "st.title(\"대화내용 보존 챗봇\")\n",
        "\n",
        "# chat history\n",
        "history = StreamlitChatMessageHistory() # StreamlitChatMessageHistory 생성\n",
        "\n",
        "# 지금까지 대화 내역 모두 복원\n",
        "for message in history.messages:\n",
        "    st.chat_message(message.type).write(message.content) # 개별 대화 내역 추가\n",
        "\n",
        "query = st.chat_input(\"하고 싶은 말\")\n",
        "\n",
        "if query:\n",
        "    with st.chat_message(\"user\"):\n",
        "        history.add_user_message(query) # 사용자가 한 말 대화 내역에 기록\n",
        "        st.markdown(query)\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        response = \"대답함\"\n",
        "        history.add_ai_message(response) # AI가 한 말 대화 내역에 기록\n",
        "        st.markdown(response)\n",
        "\n",
        "# 문제점: 항상 응답이 \"대답함\"임"
      ],
      "metadata": {
        "id": "wo7I8V4WL2lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat Model 추가하여 AI응답 만들어냄\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import streamlit as st\n",
        "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
        "# 추가\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "\n",
        "st.title(\"AI응답 챗봇\")\n",
        "\n",
        "# LLM 모델 생성\n",
        "llm = ChatOpenAI(model_name ='gpt-4o-mini', temperature=0.5)\n",
        "\n",
        "# chat history\n",
        "history = StreamlitChatMessageHistory()\n",
        "\n",
        "for message in history.messages:\n",
        "    st.chat_message(message.type).write(message.content)\n",
        "\n",
        "query = st.chat_input(\"하고 싶은 말\")\n",
        "\n",
        "if query:\n",
        "    with st.chat_message(\"user\"):\n",
        "        history.add_user_message(query)\n",
        "        st.markdown(query)\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        messages = [HumanMessage(content=query)] # 사용자 입력으로 대화내용을 만들고\n",
        "        response = llm.invoke(messages) # 그것을 바탕으로 AI응답을 얻어냄.\n",
        "        history.add_ai_message(response)\n",
        "        st.markdown(response.content)   # response.content로 변경\n",
        "\n",
        "# 문제점: '2024년 국민의힘 대표 선거 결과를 알려줘' -> '죄송하지만, 2023년 10월까지의 정보만 가지고 있으며 ...'"
      ],
      "metadata": {
        "id": "xP7vHs9MNwMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3) 고급 챗봇**"
      ],
      "metadata": {
        "id": "9zMbv3UpUoXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**외부 정보 검색 기능이 추가된 온라인 챗봇**  \n",
        "- 저장위치: **pages/2_🌐online_chatbot.py**"
      ],
      "metadata": {
        "id": "x0Pd1anphQRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 외부 정보 검색 기능 추가\n",
        "# 원달러 환율을 알려줄 수 있음\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import streamlit as st\n",
        "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "# 추가\n",
        "from langchain import hub\n",
        "from langchain_community.callbacks import StreamlitCallbackHandler\n",
        "from langchain.agents import AgentExecutor, create_openai_tools_agent, load_tools\n",
        "\n",
        "# 외부 검색 가능한 도구를 추가한 AgentExcutor 생성\n",
        "def create_agent_chain():\n",
        "    llm = ChatOpenAI(model_name ='gpt-4o', temperature=0.5)\n",
        "\n",
        "    tools = load_tools([\"ddg-search\", \"wikipedia\"])    # tools 정의\n",
        "    prompt = hub.pull(\"hwchase17/openai-tools-agent\")  # tools-agent 프롬프트 로드\n",
        "    agent = create_openai_tools_agent(llm, tools, prompt) # agent 생성\n",
        "\n",
        "    return AgentExecutor(agent=agent, tools=tools) # AgentExecutor 리턴\n",
        "\n",
        "st.set_page_config(page_title=\"온라인 챗봇\", page_icon=\"🌐\", layout='wide')\n",
        "st.header('온라인 챗봇')\n",
        "\n",
        "history = StreamlitChatMessageHistory()\n",
        "\n",
        "for message in history.messages:\n",
        "    st.chat_message(message.type).write(message.content)\n",
        "\n",
        "query = st.chat_input(\"하고 싶은 말\")\n",
        "\n",
        "if query:\n",
        "    with st.chat_message(\"user\"):\n",
        "        history.add_user_message(query)\n",
        "        st.markdown(query)\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        callback = StreamlitCallbackHandler(st.container())\n",
        "        agent_chain = create_agent_chain()\n",
        "        response = agent_chain.invoke(  # agent_chain이 응답을 반환할 때 [callback]이 호출되면서 AI의 응답이 자동으로 출력됨.\n",
        "            {\"input\": query},\n",
        "            {\"callbacks\": [callback]},\n",
        "        )\n",
        "        #messages = [HumanMessage(content=query)]  # 삭제\n",
        "        #response = llm.invoke(messages)            # 삭제\n",
        "        history.add_ai_message(response[\"output\"])\n",
        "        st.markdown(response[\"output\"])  # agent_chain의 응답이므로 변경\n",
        "\n",
        "# 문제점: 기억이 없음. 내 이름을 알려줘도 모름. 1 to 50 게임도 못함."
      ],
      "metadata": {
        "id": "lNZdveYmbzLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**기억력있는 온라인 챗봇**  \n",
        "- 저장위치: **pages/3_💽memorye chatbot.py**"
      ],
      "metadata": {
        "id": "42q0JkpHhTsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ConversationBufferMemory로 기억력 추가\n",
        "# 1 to 50 게임 가능\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import streamlit as st\n",
        "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "from langchain import hub\n",
        "from langchain_community.callbacks import StreamlitCallbackHandler\n",
        "from langchain_community.agent_toolkits.load_tools import load_tools\n",
        "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
        "# 추가\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# 외부 검색 가능한 도구를 추가한 AgentExcutor 생성\n",
        "def create_agent_chain(history): # history를 파라미터로 받음\n",
        "    llm = ChatOpenAI(model_name ='gpt-4o', temperature=0.5)\n",
        "\n",
        "    tools = load_tools([\"ddg-search\", \"wikipedia\"])\n",
        "    prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
        "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
        "\n",
        "    # 기억을 위해 ConversationBufferMemory 생성\n",
        "    memory = ConversationBufferMemory(\n",
        "        chat_memory=history, memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "    return AgentExecutor(agent=agent, tools=tools, memory=memory)  # memory 추가\n",
        "\n",
        "st.set_page_config(page_title=\"기억력 챗봇\", page_icon=\"💽\", layout='wide')\n",
        "st.header('기억력 있는 온라인 챗봇')\n",
        "\n",
        "history = StreamlitChatMessageHistory()\n",
        "\n",
        "for message in history.messages:\n",
        "    st.chat_message(message.type).write(message.content)\n",
        "\n",
        "query = st.chat_input(\"하고 싶은 말\")\n",
        "\n",
        "if query:\n",
        "    with st.chat_message(\"user\"):\n",
        "        #history.add_user_message(query)  # 삭제\n",
        "        st.markdown(query)\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        callback = StreamlitCallbackHandler(st.container())\n",
        "        agent_chain = create_agent_chain(history) # history를 파라미터로 패싱\n",
        "        response = agent_chain.invoke(\n",
        "            {\"input\": query},\n",
        "            {\"callbacks\": [callback]},\n",
        "        )\n",
        "        #history.add_ai_message(response[\"output\"]) # 삭제\n",
        "        st.markdown(response[\"output\"])  # agent_chain의 응답이므로 변경\n",
        "\n",
        "\"\"\" Test 항목\n",
        "1. 이름 기억 확인\n",
        "2. 인터넷을 통해 알 수 있는 최신 사건 질의\n",
        "   1) (아주 최근 있었던 일) 2024년 국민의힘 정당 대표는 누구야?\n",
        "   2) 지금 이 시각 청주 날씨는?\n",
        "   3) BTS 멤버의 나이는?\n",
        "3. 수학 추론 질의\n",
        "   1) 20년 후에 두 배로 돌려주는 예금 상품이 출시되었데.. 연 복리 몇% 상품인거야?\n",
        "   2) 10년만에 두배가 되려면 이율이 몇 % 여야 해?\n",
        "   4) 연복리 6%인 예금으로 10년 만에 1억을 만들려면 1년에 얼마씩 저금해야 할까?\n",
        "   5) 매년 이자의 15.4%를 이자소득세로 내야해. 다시 계산해줘\n",
        "   6) 확실해? 연도별 원금, 이자, 세금, 누적합계를 보여주는 표를 만들어줘.\n",
        "4. 기타 질문\n",
        "   1) 오늘이 몇 일이야?\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "mGTKAtDIJ6gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ReAct 버전** - PythonREPL 버그 있음"
      ],
      "metadata": {
        "id": "Q7Om8CvXz7VK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ConversationBufferMemory로 기억력 추가\n",
        "# 1 to 50 게임 가능\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import streamlit as st\n",
        "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "from langchain import hub\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "from langchain_community.callbacks import StreamlitCallbackHandler\n",
        "from langchain.agents import Tool, load_tools, create_react_agent, AgentExecutor\n",
        "# 추가\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# 외부 검색 가능한 도구를 추가한 AgentExcutor 생성\n",
        "def create_agent_chain(history): # history를 파라미터로 받음\n",
        "    llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
        "\n",
        "    tavily_tool = TavilySearchResults(k=5)\n",
        "\n",
        "    python_repl = PythonREPL()\n",
        "    python_repl_tool = Tool(\n",
        "        name=\"python_repl\",\n",
        "        description=\"A Python shell. Use this to execute python commands. \\\n",
        "        Input should be a valid python command. \\\n",
        "        If you want to see the output of a value, you should print it out with `print(...)`.\",\n",
        "        func=python_repl.run,\n",
        "    )\n",
        "\n",
        "    tools = [tavily_tool, python_repl_tool]\n",
        "\n",
        "    prompt = hub.pull(\"hwchase17/react-chat\")\n",
        "\n",
        "    # 기억을 위해 ConversationBufferMemory 생성\n",
        "    memory = ConversationBufferMemory(\n",
        "        chat_memory=history, memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "    agent = create_react_agent(llm, tools, prompt)\n",
        "    return AgentExecutor(agent=agent, tools=tools, handle_parsing_errors=True,\n",
        "                memory=memory)  # memory 추가\n",
        "\n",
        "st.set_page_config(page_title=\"메모리 챗봇\", page_icon=\"💽\", layout='wide')\n",
        "st.header('기억력 있는 온라인 챗봇')\n",
        "\n",
        "# chat history\n",
        "if('app_name' not in st.session_state):\n",
        "    st.session_state.app_name = 'memory_chatbot'\n",
        "elif(st.session_state.app_name != 'memory_chatbot'):\n",
        "    st.session_state.app_name = 'memory_chatbot'\n",
        "    StreamlitChatMessageHistory().clear();\n",
        "\n",
        "history = StreamlitChatMessageHistory()\n",
        "\n",
        "for message in history.messages:\n",
        "    st.chat_message(message.type).write(message.content)\n",
        "\n",
        "query = st.chat_input(\"하고 싶은 말\")\n",
        "\n",
        "if query:\n",
        "    with st.chat_message(\"user\"):\n",
        "        #history.add_user_message(query)  # 삭제\n",
        "        st.markdown(query)\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        callback = StreamlitCallbackHandler(st.container())\n",
        "        agent_chain = create_agent_chain(history) # history를 파라미터로 패싱\n",
        "        response = agent_chain.invoke(\n",
        "            {\"input\": query},\n",
        "            {\"callbacks\": [callback]},\n",
        "        )\n",
        "        #history.add_ai_message(response[\"output\"]) # 삭제\n",
        "        st.markdown(response[\"output\"])  # agent_chain의 응답이므로 변경"
      ],
      "metadata": {
        "id": "vMP8x18Bz6KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4) RAG 챗봇**"
      ],
      "metadata": {
        "id": "nigdIdkuiDvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RAG Q&A 앱**\n",
        "- 저장위치: **4_📄rag_chatbot1.py**"
      ],
      "metadata": {
        "id": "KABcoZ-wA4yk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG Q&A 앱 (RetrievalQA로 구현, 기억력 없음)\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import os\n",
        "import streamlit as st\n",
        "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "#추가\n",
        "from langchain_community.callbacks import StreamlitCallbackHandler\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import (\n",
        "    TextLoader, JSONLoader, UnstructuredMarkdownLoader, PyMuPDFLoader)\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "import faiss\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore import InMemoryDocstore\n",
        "\n",
        "st.title(\"RAG Q&A 앱\")\n",
        "\n",
        "embedding_model = OpenAIEmbeddings()\n",
        "# FAISS vector store 관련\n",
        "vs = FAISS(\n",
        "    embedding_function=embedding_model,\n",
        "    index=faiss.IndexFlatL2(1536),\n",
        "    docstore=InMemoryDocstore(), index_to_docstore_id={})\n",
        "\n",
        "def vs_add_file(file_path):\n",
        "    if file_path.endswith('.txt'):\n",
        "        text_loader = TextLoader(file_path)\n",
        "        raw_doc = text_loader.load()\n",
        "    elif file_path.endswith('.md'):\n",
        "        markdown_loader = UnstructuredMarkdownLoader(file_path)\n",
        "        raw_doc = markdown_loader.load()\n",
        "    elif file_path.endswith('.pdf'):\n",
        "        pdf_loader = PyMuPDFLoader(file_path)\n",
        "        raw_doc = pdf_loader.load()\n",
        "    elif file_path.endswith('.json'):\n",
        "        json_loader = JSONLoader(file_path)\n",
        "        raw_doc = json_loader.load()\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size = 1000, chunk_overlap = 200)\n",
        "    docs = text_splitter.split_documents(raw_doc)\n",
        "\n",
        "    vs.add_documents(docs)\n",
        "\n",
        "def save_file(file):\n",
        "    import os\n",
        "    folder = 'tmp'\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    file_path = f'./{folder}/{file.name}'\n",
        "    with open(file_path, 'wb') as f:\n",
        "        f.write(file.getvalue())\n",
        "    return file_path\n",
        "\n",
        "uploaded_files = st.file_uploader(\"Choose a data file\", accept_multiple_files=True)\n",
        "for file in uploaded_files:\n",
        "    file_path = save_file(file)\n",
        "    vs_add_file(file_path)\n",
        "\n",
        "# retriever\n",
        "def create_retriever():\n",
        "    gpt4o = ChatOpenAI(model_name ='gpt-4o-mini', temperature=0.5)\n",
        "\n",
        "    retriever = RetrievalQA.from_chain_type(\n",
        "        llm=gpt4o,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=vs.as_retriever(\n",
        "            search_type='mmr',\n",
        "            search_kwargs={'k':8, 'fetch_k':12}\n",
        "        ),\n",
        "        return_source_documents=True\n",
        "    )\n",
        "    return retriever\n",
        "\n",
        "# 쿼리 및 응답 처리\n",
        "query = st.chat_input(\"하고 싶은 말\")\n",
        "\n",
        "if query:\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(query)\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        callback = StreamlitCallbackHandler(st.container())\n",
        "\n",
        "        retriever = create_retriever()\n",
        "        response = retriever.invoke(\n",
        "            {\"query\": query},\n",
        "            {\"callbacks\": [callback]},\n",
        "        )\n",
        "        st.markdown(response[\"result\"])\n",
        "\n",
        "        # to show references\n",
        "        for idx, doc in enumerate(response['source_documents'],1):\n",
        "            filename = os.path.basename(doc.metadata['source'])\n",
        "            ref_title = f\":blue[Reference {idx}: *{filename}*]\"\n",
        "            with st.popover(ref_title):\n",
        "                st.caption(doc.page_content)\n",
        "\n",
        "\"\"\"\n",
        "1. 윤초시와 소녀는 어떤 관계인가?\n",
        "2. 소설의 끝에서 소녀는 어떠한 결말을 맞이해? 근거를 들어서 설명해줘.\"\"\""
      ],
      "metadata": {
        "id": "awpIqUPPB4dE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RAG 챗봇 (ConversationalRetrievalChain 구현)** - Chroma로 구현"
      ],
      "metadata": {
        "id": "uiw0J8hJy_YL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 실습 PASS\n",
        "# RAG 챗봇 (ConversationalRetrievalChain 구현)\n",
        "# ConversationalRetrievalChain을 사용함에도 불구하고 기억력 유지가 안됨\n",
        "# 이유는 알지만 쉽게 해결하는 방법을 못찾겠음\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import os\n",
        "import streamlit as st\n",
        "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "from langchain import hub\n",
        "from langchain_community.callbacks import StreamlitCallbackHandler\n",
        "from langchain_community.agent_toolkits.load_tools import load_tools\n",
        "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "#추가\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import (\n",
        "    TextLoader, JSONLoader, UnstructuredMarkdownLoader, PyMuPDFLoader)\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "st.title(\"RAG 챗봇\")\n",
        "\n",
        "# vector store 관련\n",
        "db_dir = \"chroma-db/\"\n",
        "embedding_model = OpenAIEmbeddings()\n",
        "\n",
        "vs = Chroma(\"langchain_store\", embedding_model, persist_directory = db_dir)\n",
        "\n",
        "def vs_add_file(file_path):\n",
        "    if file_path.endswith('.txt'):\n",
        "        text_loader = TextLoader(file_path)\n",
        "        raw_doc = text_loader.load()\n",
        "    elif file_path.endswith('.md'):\n",
        "        markdown_loader = UnstructuredMarkdownLoader(file_path)\n",
        "        raw_doc = markdown_loader.load()\n",
        "    elif file_path.endswith('.pdf'):\n",
        "        pdf_loader = PyMuPDFLoader(file_path)\n",
        "        raw_doc = pdf_loader.load()\n",
        "    elif file_path.endswith('.json'):\n",
        "        json_loader = JSONLoader(file_path)\n",
        "        raw_doc = json_loader.load()\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size = 1000, chunk_overlap  = 200)\n",
        "    docs = text_splitter.split_documents(raw_doc)\n",
        "\n",
        "    vs.add_documents(docs)\n",
        "\n",
        "def save_file(file):\n",
        "    import os\n",
        "    folder = 'tmp'\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    file_path = f'./{folder}/{file.name}'\n",
        "    with open(file_path, 'wb') as f:\n",
        "        f.write(file.getvalue())\n",
        "    return file_path\n",
        "\n",
        "uploaded_files = st.file_uploader(\"Choose a data file\", accept_multiple_files=True)\n",
        "for file in uploaded_files:\n",
        "    file_path = save_file(file)\n",
        "    vs_add_file(file_path)\n",
        "\n",
        "# history\n",
        "history = StreamlitChatMessageHistory()\n",
        "\n",
        "for message in history.messages:\n",
        "    st.chat_message(message.type).write(message.content)\n",
        "\n",
        "# chains\n",
        "def create_qa_chain(history):\n",
        "    gpt4o = ChatOpenAI(model_name ='gpt-4o-mini', temperature=0.5)\n",
        "\n",
        "    memory = ConversationBufferMemory(\n",
        "        chat_memory=history,  memory_key=\"chat_history\",\n",
        "        input_key='question', output_key='answer',\n",
        "        return_messages=True)\n",
        "\n",
        "    retriever = vs.as_retriever(\n",
        "            search_type='mmr',\n",
        "            search_kwargs={'k':2, 'fetch_k':4}\n",
        "        )\n",
        "\n",
        "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "            llm = gpt4o,\n",
        "            retriever = retriever,\n",
        "            memory = memory,\n",
        "            return_source_documents=True,\n",
        "            verbose=True\n",
        "        )\n",
        "    return qa_chain\n",
        "\n",
        "#\n",
        "query = st.chat_input(\"하고 싶은 말\")\n",
        "\n",
        "if query:\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(query)\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        callback = StreamlitCallbackHandler(st.container())\n",
        "\n",
        "        qa_chain = create_qa_chain(history)\n",
        "        response = qa_chain.invoke(\n",
        "            {\"question\": query},\n",
        "            {\"callbacks\": [callback]},\n",
        "        )\n",
        "        st.markdown(response[\"answer\"])\n",
        "\n",
        "        # to show references\n",
        "        for idx, doc in enumerate(response['source_documents'],1):\n",
        "            print(doc)\n",
        "            filename = os.path.basename(doc.metadata['source'])\n",
        "            ref_title = f\":blue[Reference {idx}: *{filename}*]\"\n",
        "            with st.popover(ref_title):\n",
        "                st.caption(doc.page_content)"
      ],
      "metadata": {
        "id": "D9gw9yEJsFHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RAG 챗봇 (create_history_aware_retriever 활용)**  - FAISS로 구현\n",
        "- 저장위치: **4_📄rag_chatbot2.py**"
      ],
      "metadata": {
        "id": "-WglZ5OZAzzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG 챗봇 (create_history_aware_retriever 구현)\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import os\n",
        "import streamlit as st\n",
        "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "#추가\n",
        "from langchain_community.callbacks import StreamlitCallbackHandler\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import (\n",
        "    TextLoader, JSONLoader, UnstructuredMarkdownLoader, PyMuPDFLoader)\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.outputs import LLMResult\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "import faiss\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore import InMemoryDocstore\n",
        "st.set_page_config(page_title=\"RAG 챗봇\", page_icon=\"📄\", layout='wide')\n",
        "st.header('RAG 챗봇')\n",
        "\n",
        "embedding_model = OpenAIEmbeddings()\n",
        "\n",
        "# FAISS vector store 관련\n",
        "vs = FAISS(\n",
        "    embedding_function=embedding_model,\n",
        "    index=faiss.IndexFlatL2(1536),\n",
        "    docstore=InMemoryDocstore(), index_to_docstore_id={})\n",
        "\n",
        "def vs_add_file(file_path):\n",
        "    if file_path.endswith('.txt'):\n",
        "        text_loader = TextLoader(file_path)\n",
        "        raw_doc = text_loader.load()\n",
        "    elif file_path.endswith('.md'):\n",
        "        markdown_loader = UnstructuredMarkdownLoader(file_path)\n",
        "        raw_doc = markdown_loader.load()\n",
        "    elif file_path.endswith('.pdf'):\n",
        "        pdf_loader = PyMuPDFLoader(file_path)\n",
        "        raw_doc = pdf_loader.load()\n",
        "    elif file_path.endswith('.json'):\n",
        "        json_loader = JSONLoader(file_path)\n",
        "        raw_doc = json_loader.load()\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size = 500, chunk_overlap  = 100)\n",
        "    docs = text_splitter.split_documents(raw_doc)\n",
        "\n",
        "    vs.add_documents(docs)\n",
        "\n",
        "def save_file(file):\n",
        "    import os\n",
        "    folder = 'tmp'\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    file_path = f'./{folder}/{file.name}'\n",
        "    with open(file_path, 'wb') as f:\n",
        "        f.write(file.getvalue())\n",
        "    return file_path\n",
        "\n",
        "uploaded_files = st.file_uploader(\"Choose a data file\", accept_multiple_files=True)\n",
        "for file in uploaded_files:\n",
        "    file_path = save_file(file)\n",
        "    vs_add_file(file_path)\n",
        "\n",
        "# chat history\n",
        "if('app_name' not in st.session_state):\n",
        "    st.session_state.app_name = 'rag_chatbot'\n",
        "elif(st.session_state.app_name != 'rag_chatbot'):\n",
        "    st.session_state.app_name = 'rag_chatbot'\n",
        "    StreamlitChatMessageHistory().clear();\n",
        "\n",
        "history = StreamlitChatMessageHistory()\n",
        "\n",
        "for message in history.messages:\n",
        "    st.chat_message(message.type).write(message.content)\n",
        "\n",
        "# create_retriever_chain\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "def create_retriever_chain(history):\n",
        "    # LangChain의 create_history_aware_retriever를 사용해,\n",
        "    # 과거의 대화 기록을 고려해 질문을 다시 표현하는 Chain을 생성\n",
        "    rephrase_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "            (\"user\", \"{input}\"),\n",
        "            (\"user\", \"위의 대화에서, 대화와 관련된 정보를 찾기 위한 검색 쿼리를 생성해 주세요.\"),\n",
        "        ]\n",
        "    )\n",
        "    rephrase_llm = ChatOpenAI(model_name ='gpt-4o-mini', temperature=0.5)\n",
        "\n",
        "    retriever=vs.as_retriever(\n",
        "            search_type='mmr',\n",
        "            search_kwargs={'k':8, 'fetch_k':12}\n",
        "    )\n",
        "\n",
        "    rephrase_chain = create_history_aware_retriever(\n",
        "        rephrase_llm, retriever, rephrase_prompt\n",
        "    )\n",
        "\n",
        "    qa_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", \"아래의 문맥만을 고려하여 질문에 답하세요.\\n\\n{context}\"),\n",
        "            (MessagesPlaceholder(variable_name=\"chat_history\")),\n",
        "            (\"user\", \"{input}\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    callback = StreamlitCallbackHandler(st.container())\n",
        "\n",
        "    qa_llm = ChatOpenAI(\n",
        "        model_name ='gpt-4o',\n",
        "        temperature=0.5,\n",
        "        streaming=True,\n",
        "        callbacks=[callback]\n",
        "    )\n",
        "\n",
        "    qa_chain = qa_prompt | qa_llm | StrOutputParser()\n",
        "\n",
        "    # 두 Chain을 연결한 Chain을 생성\n",
        "    conversational_retrieval_chain = (\n",
        "        RunnablePassthrough.assign(context=rephrase_chain | format_docs) | qa_chain\n",
        "    )\n",
        "\n",
        "    return conversational_retrieval_chain\n",
        "\n",
        "# 쿼리 및 응답\n",
        "query = st.chat_input(\"하고 싶은 말\")\n",
        "\n",
        "if query:\n",
        "    with st.chat_message(\"user\"):\n",
        "        history.add_user_message(query)\n",
        "        st.markdown(query)\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        #callback = StreamlitCallbackHandler(st.container())\n",
        "\n",
        "        retriever = create_retriever_chain(history)\n",
        "        response = retriever.invoke(\n",
        "            {\"input\": query, \"chat_history\": history.messages},\n",
        "            #{\"callbacks\": [callback]},\n",
        "        )\n",
        "        history.add_ai_message(response)\n",
        "        st.markdown(response)\n",
        "\n",
        "\"\"\"\n",
        "1. 한국사.pdf\n",
        "  (출력 토큰 수 제한으로 한꺼번에 다 물어보면 안됨)\n",
        "  1,2,3,4,5번 문제를 예쁘게 출력하고 정답과 그 이유를 알려줘\n",
        "2. [만숑쌤]한국사 막판 키워드 정리(배포용).pdf\n",
        "   1) 성종이 한일은?\n",
        "   2) '아니 조선시대' or '아니 고려시대'\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "aAcKkGOoAlqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LangGraph 이용 버전**"
      ],
      "metadata": {
        "id": "0cAT6W-zKmtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG 챗봇 (LangGraph)\n",
        "# 소스코드를 더 다듬어야 함.\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import bs4\n",
        "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "from langchain.tools.retriever import create_retriever_tool\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langgraph.checkpoint.sqlite import SqliteSaver\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "### Construct retriever ###\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "\n",
        "### Build retriever tool ###\n",
        "tool = create_retriever_tool(\n",
        "    retriever,\n",
        "    \"blog_post_retriever\",\n",
        "    \"Searches and returns excerpts from the Autonomous Agents blog post.\",\n",
        ")\n",
        "tools = [tool]\n",
        "\n",
        "agent_executor = create_react_agent(llm, tools, checkpointer=memory)\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
        "response = agent_executor.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\"Autonomous Agents에서 Self-Reflection이 뭐야? 한글로 설명해줘.\")]}, config\n",
        ")\n",
        "print(response[\"messages\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJkrAH5xDEXn",
        "outputId": "43388a2f-89ba-4785-9d60-7e5b86f0ad89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HumanMessage(content='Autonomous Agents에서 Self-Reflection이 뭐야? 한글로 설명해줘.', id='a841438c-f1ba-4001-aeb8-1cfe5932b4e5'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_8RqXPkLd1xvi7phqrMmTcu6L', 'function': {'arguments': '{\"query\":\"Self-Reflection\"}', 'name': 'blog_post_retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 78, 'total_tokens': 97}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_661538dc1f', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-f40106e8-a76a-4f8d-9c50-2eb33009d0ca-0', tool_calls=[{'name': 'blog_post_retriever', 'args': {'query': 'Self-Reflection'}, 'id': 'call_8RqXPkLd1xvi7phqrMmTcu6L', 'type': 'tool_call'}], usage_metadata={'input_tokens': 78, 'output_tokens': 19, 'total_tokens': 97}), ToolMessage(content='Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\n\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\n\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\n\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.', name='blog_post_retriever', id='11bd35e2-3ef2-43a5-a448-c683462a48c4', tool_call_id='call_8RqXPkLd1xvi7phqrMmTcu6L'), AIMessage(content='자율 에이전트에서의 자기 반성(Self-Reflection)은 매우 중요한 요소로, 에이전트가 과거의 행동 결정을 개선하고 이전의 실수를 수정함으로써 점진적으로 발전할 수 있도록 합니다. 이는 시행착오가 불가피한 실제 작업에서 중요한 역할을 합니다. 자기 반성을 통해 에이전트는 자신의 경험을 분석하고, 이를 바탕으로 더 나은 결정을 내릴 수 있게 됩니다.', response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 884, 'total_tokens': 982}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_661538dc1f', 'finish_reason': 'stop', 'logprobs': None}, id='run-b4571c74-8f5e-4789-8de6-24848c9f610e-0', usage_metadata={'input_tokens': 884, 'output_tokens': 98, 'total_tokens': 982})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5) 인공지능 보조교사 (FAISS ver.)**"
      ],
      "metadata": {
        "id": "IL5zJO8g1Dbe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**프로그래밍 인공지능 보조교사 구현**  \n",
        "- 저장위치: **5_⭐coteacher.py**"
      ],
      "metadata": {
        "id": "719UPC4F1cmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import os\n",
        "import streamlit as st\n",
        "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "#추가\n",
        "from langchain_community.callbacks import StreamlitCallbackHandler\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.outputs import LLMResult\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "import faiss\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore import InMemoryDocstore\n",
        "\n",
        "st.set_page_config(page_title=\"인공지능 보조교사\", page_icon=\"⭐\", layout='wide')\n",
        "st.header('프로그래밍 인공지능 보조교사')\n",
        "\n",
        "embedding_model = OpenAIEmbeddings()\n",
        "# FAISS vector store 관련\n",
        "faiss_dir = './faiss'\n",
        "if(os.path.isdir(faiss_dir) == False):\n",
        "    vs = FAISS(\n",
        "            embedding_function=embedding_model, index=faiss.IndexFlatL2(1536),\n",
        "            docstore=InMemoryDocstore(), index_to_docstore_id={})\n",
        "else :\n",
        "    vs = FAISS.load_local(faiss_dir, embedding_model,\n",
        "            allow_dangerous_deserialization=True)\n",
        "\n",
        "# chat history\n",
        "if('app_name' not in st.session_state):\n",
        "    st.session_state.app_name = 'coteacher'\n",
        "elif(st.session_state.app_name != 'coteacher'):\n",
        "    st.session_state.app_name = 'coteacher'\n",
        "    StreamlitChatMessageHistory().clear();\n",
        "\n",
        "history = StreamlitChatMessageHistory()\n",
        "\n",
        "if len(history.messages) == 0:  # 대화내역이 전무하다면...\n",
        "    hello = \"안녕하세요? 무슨이야기를 해볼까요?\"\n",
        "    history.add_ai_message(hello)\n",
        "\n",
        "for message in history.messages:\n",
        "    st.chat_message(message.type).write(message.content)\n",
        "\n",
        "# create_retriever_chain\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "def create_retriever_chain(history):\n",
        "    # LangChain의 create_history_aware_retriever를 사용해,\n",
        "    # 과거의 대화 기록을 고려해 질문을 다시 표현하는 Chain을 생성\n",
        "    rephrase_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "            (\"user\", \"{input}\"),\n",
        "            (\"user\", \"위의 대화에서, 대화와 관련된 정보를 찾기 위한 검색 쿼리를 생성해 주세요.\"),\n",
        "        ]\n",
        "    )\n",
        "    rephrase_llm = ChatOpenAI(model_name ='gpt-4o-mini', temperature=0.5)\n",
        "\n",
        "    retriever=vs.as_retriever(\n",
        "            search_type='mmr',\n",
        "            search_kwargs={'k':4, 'fetch_k':8}\n",
        "    )\n",
        "\n",
        "    rephrase_chain = create_history_aware_retriever(\n",
        "        rephrase_llm, retriever, rephrase_prompt\n",
        "    )\n",
        "\n",
        "    coteacher_prompt = \"\"\"\n",
        "    You are an assistant teacher teaching programming desigend by 정진쌤.\n",
        "    Please answer the student's questions appropriately.\n",
        "    However, refuse requests to provide the correct answer code, to create a program, or to provide sample code.\n",
        "    If requested, you can provide a psudo-code.\n",
        "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "    Use the following context to answer the question at the end.\n",
        "    Please answer in Korean unless otherwise requested.\\n\n",
        "    {context}\n",
        "    \"\"\"\n",
        "\n",
        "    qa_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", coteacher_prompt),\n",
        "            (MessagesPlaceholder(variable_name=\"chat_history\")),\n",
        "            (\"user\", \"{input}\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    callback = StreamlitCallbackHandler(st.container())\n",
        "\n",
        "    qa_llm = ChatOpenAI(\n",
        "        model_name ='gpt-4o-mini',\n",
        "        temperature=0.5,\n",
        "        streaming=True,\n",
        "        callbacks=[callback]\n",
        "    )\n",
        "\n",
        "    qa_chain = qa_prompt | qa_llm | StrOutputParser()\n",
        "\n",
        "    # 두 Chain을 연결한 Chain을 생성\n",
        "    conversational_retrieval_chain = (\n",
        "        RunnablePassthrough.assign(context=rephrase_chain | format_docs) | qa_chain\n",
        "    )\n",
        "\n",
        "    return conversational_retrieval_chain\n",
        "\n",
        "# 쿼리 및 응답\n",
        "query = st.chat_input(\"하고 싶은 말\")\n",
        "\n",
        "if query:\n",
        "    with st.chat_message(\"user\"):\n",
        "        history.add_user_message(query)\n",
        "        st.markdown(query)\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        #callback = StreamlitCallbackHandler(st.container())\n",
        "\n",
        "        retriever = create_retriever_chain(history)\n",
        "        response = retriever.invoke(\n",
        "            {\"input\": query, \"chat_history\": history.messages},\n",
        "            #{\"callbacks\": [callback]},\n",
        "        )\n",
        "        history.add_ai_message(response)\n",
        "        st.markdown(response)"
      ],
      "metadata": {
        "id": "d17uRScc1Jy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**인공지능 보조교사 Knowlegebase 생성기**\n",
        "- 저장위치: **6_🎓knowlegebase.py**"
      ],
      "metadata": {
        "id": "hB85DGn11soY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import os\n",
        "import streamlit as st\n",
        "#추가\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import (\n",
        "    TextLoader, JSONLoader, UnstructuredMarkdownLoader, PyMuPDFLoader)\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "import faiss\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore import InMemoryDocstore\n",
        "\n",
        "st.set_page_config(page_title=\"챗봇\", page_icon=\"⭐\", layout='wide')\n",
        "st.header('인공지능 보조교사')\n",
        "st.markdown(\"#### **Knowledgebase 생성기**\")\n",
        "\n",
        "embedding_model = OpenAIEmbeddings()\n",
        "# FAISS vector store 관련\n",
        "faiss_dir = './faiss'\n",
        "if(os.path.isdir(faiss_dir) == False):\n",
        "    vs = FAISS(\n",
        "            embedding_function=embedding_model, index=faiss.IndexFlatL2(1536),\n",
        "            docstore=InMemoryDocstore(), index_to_docstore_id={})\n",
        "else :\n",
        "    vs = FAISS.load_local(faiss_dir, embedding_model,\n",
        "            allow_dangerous_deserialization=True)\n",
        "\n",
        "def vs_add_file(file_path):\n",
        "    if file_path.endswith('.txt'):\n",
        "        text_loader = TextLoader(file_path)\n",
        "        raw_doc = text_loader.load()\n",
        "    elif file_path.endswith('.md'):\n",
        "        markdown_loader = UnstructuredMarkdownLoader(file_path)\n",
        "        raw_doc = markdown_loader.load()\n",
        "    elif file_path.endswith('.pdf'):\n",
        "        pdf_loader = PyMuPDFLoader(file_path)\n",
        "        raw_doc = pdf_loader.load()\n",
        "    elif file_path.endswith('.json'):\n",
        "        json_loader = JSONLoader(file_path)\n",
        "        raw_doc = json_loader.load()\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size = 500, chunk_overlap  = 100)\n",
        "    docs = text_splitter.split_documents(raw_doc)\n",
        "\n",
        "    if(len(docs)>0):\n",
        "        vs.add_documents(docs)\n",
        "\n",
        "    vs.save_local(faiss_dir)\n",
        "\n",
        "def save_file(file):\n",
        "    import os\n",
        "    folder = 'tmp'\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    file_path = f'./{folder}/{file.name}'\n",
        "    with open(file_path, 'wb') as f:\n",
        "        f.write(file.getvalue())\n",
        "    return file_path\n",
        "\n",
        "uploaded_files = st.file_uploader(\"Choose a data file\", accept_multiple_files=True)\n",
        "for file in uploaded_files:\n",
        "    file_path = save_file(file)\n",
        "    vs_add_file(file_path)\n",
        "\n",
        "if(len(uploaded_files)):\n",
        "    st.markdown(\"#### 업로드 완료 되었습니다.\")"
      ],
      "metadata": {
        "id": "eQ9KiSY01oaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5) 인공지능 보조교사 (Chroma ver.)**"
      ],
      "metadata": {
        "id": "JZdV8hdDBrH1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**프로그래밍 인공지능 보조교사 구현**  "
      ],
      "metadata": {
        "id": "fwnpzZhGYWta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 보조교사 챗봇\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import os\n",
        "import streamlit as st\n",
        "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "#추가\n",
        "from langchain_community.callbacks import StreamlitCallbackHandler\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.outputs import LLMResult\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "st.set_page_config(page_title=\"챗봇\", page_icon=\"⭐\", layout='wide')\n",
        "st.header('프로그래밍 인공지능 보조교사')\n",
        "\n",
        "# vector store 관련\n",
        "db_dir = \"chroma-db/\"\n",
        "embedding_model = OpenAIEmbeddings()\n",
        "\n",
        "vs = Chroma(\"langchain_store\", embedding_model, persist_directory = db_dir)\n",
        "\n",
        "# chat history\n",
        "if('app_name' not in st.session_state):\n",
        "    st.session_state.app_name = 'coteacher'\n",
        "elif(st.session_state.app_name != 'coteacher'):\n",
        "    st.session_state.app_name = 'coteacher'\n",
        "    StreamlitChatMessageHistory().clear();\n",
        "\n",
        "history = StreamlitChatMessageHistory()\n",
        "\n",
        "if len(history.messages) == 0:  # 대화내역이 전무하다면...\n",
        "    hello = \"안녕하세요? 무슨이야기를 해볼까요?\"\n",
        "    history.add_ai_message(hello)\n",
        "\n",
        "for message in history.messages:\n",
        "    st.chat_message(message.type).write(message.content)\n",
        "\n",
        "# create_retriever_chain\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "def create_retriever_chain(history):\n",
        "    # LangChain의 create_history_aware_retriever를 사용해,\n",
        "    # 과거의 대화 기록을 고려해 질문을 다시 표현하는 Chain을 생성\n",
        "    rephrase_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "            (\"user\", \"{input}\"),\n",
        "            (\"user\", \"위의 대화에서, 대화와 관련된 정보를 찾기 위한 검색 쿼리를 생성해 주세요.\"),\n",
        "        ]\n",
        "    )\n",
        "    rephrase_llm = ChatOpenAI(model_name ='gpt-4o-mini', temperature=0.5)\n",
        "\n",
        "    retriever=vs.as_retriever(\n",
        "            search_type='mmr',\n",
        "            search_kwargs={'k':2, 'fetch_k':4}\n",
        "    )\n",
        "\n",
        "    rephrase_chain = create_history_aware_retriever(\n",
        "        rephrase_llm, retriever, rephrase_prompt\n",
        "    )\n",
        "\n",
        "    coteacher_prompt = \"\"\"\n",
        "    You are an assistant teacher teaching programming desigend by 정진쌤.\n",
        "    Please answer the student's questions appropriately.\n",
        "    However, refuse requests to provide the correct answer code, to create a program, or to provide sample code.\n",
        "    If requested, you can provide a psudo-code.\n",
        "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "    Use the following context to answer the question at the end.\n",
        "    Please answer in Korean unless otherwise requested.\\n\n",
        "    {context}\n",
        "    \"\"\"\n",
        "\n",
        "    qa_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", coteacher_prompt),\n",
        "            (MessagesPlaceholder(variable_name=\"chat_history\")),\n",
        "            (\"user\", \"{input}\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    callback = StreamlitCallbackHandler(st.container())\n",
        "\n",
        "    qa_llm = ChatOpenAI(\n",
        "        model_name ='gpt-4o-mini',\n",
        "        temperature=0.5,\n",
        "        streaming=True,\n",
        "        callbacks=[callback]\n",
        "    )\n",
        "\n",
        "    qa_chain = qa_prompt | qa_llm | StrOutputParser()\n",
        "\n",
        "    # 두 Chain을 연결한 Chain을 생성\n",
        "    conversational_retrieval_chain = (\n",
        "        RunnablePassthrough.assign(context=rephrase_chain | format_docs) | qa_chain\n",
        "    )\n",
        "\n",
        "    return conversational_retrieval_chain\n",
        "\n",
        "# 쿼리 및 응답\n",
        "query = st.chat_input(\"하고 싶은 말\")\n",
        "\n",
        "if query:\n",
        "    with st.chat_message(\"user\"):\n",
        "        history.add_user_message(query)\n",
        "        st.markdown(query)\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        #callback = StreamlitCallbackHandler(st.container())\n",
        "\n",
        "        retriever = create_retriever_chain(history)\n",
        "        response = retriever.invoke(\n",
        "            {\"input\": query, \"chat_history\": history.messages},\n",
        "            #{\"callbacks\": [callback]},\n",
        "        )\n",
        "        history.add_ai_message(response)\n",
        "        st.markdown(response)"
      ],
      "metadata": {
        "id": "_lCWJJ5Y4SKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**인공지능 보조교사 Knowlegebase 생성기**"
      ],
      "metadata": {
        "id": "uRMl09A2SR4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import os\n",
        "import streamlit as st\n",
        "#추가\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import (\n",
        "    TextLoader, JSONLoader, UnstructuredMarkdownLoader, PyMuPDFLoader)\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "st.set_page_config(page_title=\"챗봇\", page_icon=\"⭐\", layout='wide')\n",
        "st.header('인공지능 보조교사')\n",
        "st.markdown(\"#### **Knowledgebase 생성기**\")\n",
        "\n",
        "# vector store 관련\n",
        "db_dir = \"chroma-db/\"\n",
        "embedding_model = OpenAIEmbeddings()\n",
        "\n",
        "vs = Chroma(\"langchain_store\", embedding_model, persist_directory = db_dir)\n",
        "\n",
        "def vs_add_file(file_path):\n",
        "    if file_path.endswith('.txt'):\n",
        "        text_loader = TextLoader(file_path)\n",
        "        raw_doc = text_loader.load()\n",
        "    elif file_path.endswith('.md'):\n",
        "        markdown_loader = UnstructuredMarkdownLoader(file_path)\n",
        "        raw_doc = markdown_loader.load()\n",
        "    elif file_path.endswith('.pdf'):\n",
        "        pdf_loader = PyMuPDFLoader(file_path)\n",
        "        raw_doc = pdf_loader.load()\n",
        "    elif file_path.endswith('.json'):\n",
        "        json_loader = JSONLoader(file_path)\n",
        "        raw_doc = json_loader.load()\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size = 1000, chunk_overlap  = 200)\n",
        "    docs = text_splitter.split_documents(raw_doc)\n",
        "\n",
        "    if(len(docs)>0):\n",
        "        vs.add_documents(docs)\n",
        "\n",
        "def save_file(file):\n",
        "    import os\n",
        "    folder = 'tmp'\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    file_path = f'./{folder}/{file.name}'\n",
        "    with open(file_path, 'wb') as f:\n",
        "        f.write(file.getvalue())\n",
        "    return file_path\n",
        "\n",
        "uploaded_files = st.file_uploader(\"Choose a data file\", accept_multiple_files=True)\n",
        "for file in uploaded_files:\n",
        "    file_path = save_file(file)\n",
        "    vs_add_file(file_path)\n",
        "\n",
        "if(len(uploaded_files)):\n",
        "    st.markdown(\"#### 업로드 완료 되었습니다.\")"
      ],
      "metadata": {
        "id": "hlTkOhu9SIJT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}