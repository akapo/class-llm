{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMy6r89vY8FMLhCxKcYGT6F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akapo/class-llm/blob/main/chapter3/llm_app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ì‹¤ìŠµ í™˜ê²½ ì„¤ì •"
      ],
      "metadata": {
        "id": "1Qdc2CmbE69p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU openai\n",
        "!pip install -qU langchain langchain-openai langchain-community langchainhub langchain-experimental langgraph\n",
        "!pip install -qU huggingface_hub langchain-google-genai langchain-anthropic\n",
        "!pip install -Uq sentence-transformers\n",
        "!pip install -qU python-dotenv\n",
        "!pip install -qU pymupdf pypdf unstructured markdown\n",
        "!pip install -qU numexpr\n",
        "!pip install -qU faiss-cpu\n",
        "!pip install -qU chromadb\n",
        "!pip install -qU bs4\n",
        "!pip install -qU google-search-results duckduckgo-search\n",
        "!pip install -qU streamlit\n",
        "!pip install -qU streamlit-chat"
      ],
      "metadata": {
        "id": "lvBig087ErWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "langchain.debug = False\n",
        "langchain.verbose = False\n",
        "\n",
        "# LangSmith ë¥¼ ì´ìš©í•œ ëª¨ë‹ˆí„°ë§\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"chain-monitor\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"]=\"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\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\""
      ],
      "metadata": {
        "id": "NCEJXTmhBeMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LLMí™œìš© ì¸ê³µì§€ëŠ¥ ì•± ë§Œë“¤ê¸°**"
      ],
      "metadata": {
        "id": "gmXXlTEmhzF2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Web index íŒŒì¼**  \n",
        "- ì €ì¥ìœ„ì¹˜: **home.py**"
      ],
      "metadata": {
        "id": "dJCqm3nqItdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"Langchain Streamlit App Examples\",\n",
        "    page_icon='ğŸ’¬',\n",
        "    layout='wide'\n",
        ")\n",
        "\n",
        "st.header(\"Chatbot Implementations with Langchain + Streamlit\")\n",
        "st.write(\"\"\"\n",
        "[![view source code ](https://img.shields.io/badge/GitHub%20Repository-gray?logo=github)](https://github.com/akapo/llm-app)\n",
        "\"\"\")\n",
        "st.write(\"\"\"\n",
        "Langchainì€ LLM(ì–¸ì–´ ëª¨ë¸)ì„ ì‚¬ìš©í•˜ì—¬ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ê°„ì†Œí™”í•˜ë„ë¡ ì„¤ê³„ëœ ê°•ë ¥í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ êµ¬ì„± ìš”ì†Œì˜ í¬ê´„ì ì¸ í†µí•©ì„ ì œê³µí•˜ì—¬ ê°•ë ¥í•œ ì‘ìš© í”„ë¡œê·¸ë¨ì„ ë§Œë“¤ê¸° ìœ„í•´ ì¡°ë¦½ í”„ë¡œì„¸ìŠ¤ë¥¼ ë‹¨ìˆœí™”í•©ë‹ˆë‹¤.\n",
        "\n",
        "Langchainì˜ í˜ì„ í™œìš©í•˜ë©´ ì±—ë´‡ ìƒì„±ì´ ì‰¬ì›Œì§‘ë‹ˆë‹¤. ë‹¤ìŒì€ ë‹¤ì–‘í•œ ì‚¬ìš© ì‚¬ë¡€ì— ë§ëŠ” ì±—ë´‡ êµ¬í˜„ì˜ ëª‡ ê°€ì§€ ì˜ˆì…ë‹ˆë‹¤.\n",
        "\n",
        "- **ğŸ’¬translato**: ë²ˆì—­ ì„œë¹„ìŠ¤ ì•±(ë‹¤ì–‘í•œ ì–¸ì–´ ì§€ì›).\n",
        "- **online_chatbot**: ì¸í„°ë„·ì— ì ‘ì†í•˜ì—¬ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ëŠ” ì±—ë´‡ êµ¬í˜„.\n",
        "- **ğŸ’½memorye chatbot**: ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ ì§€í•˜ë©° ê¸°ì–µë ¥ì„ ê°€ì§€ëŠ” ì±—ë´‡ êµ¬í˜„.\n",
        "- **ğŸ“„rag_chatbot**: ì…ë ¥í•´ì¤€ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì±—ë´‡ êµ¬í˜„.\n",
        "- **â­coteacher**: í”„ë¡œê·¸ë˜ë° ì¸ê³µì§€ëŠ¥ ë³´ì¡°êµì‚¬ êµ¬í˜„.\n",
        "- **ğŸ“Knowlegebase**: ì¸ê³µì§€ëŠ¥ ë³´ì¡°êµì‚¬ì—ê²Œ ì§€ì‹ ì£¼ì….\n",
        "\n",
        "ê° ì±—ë´‡ì˜ ìƒ˜í”Œ ì‚¬ìš©ë²•ì„ ì‚´í´ë³´ë ¤ë©´ í•´ë‹¹ ì±—ë´‡ ì„¹ì…˜ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”.\"\"\")"
      ],
      "metadata": {
        "id": "kJ4c6vOJCuVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1) ë²ˆì—­ ì„œë¹„ìŠ¤ ì•±**"
      ],
      "metadata": {
        "id": "Ubd7dP3mOx9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ë²ˆì—­ ì„œë¹„ìŠ¤ ì•±**  \n",
        "- ì €ì¥ìœ„ì¹˜: **pages/1_ğŸ’¬translator.py**"
      ],
      "metadata": {
        "id": "wDiIvahuJYXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import streamlit as st\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "langs = [\"English\", \"Japanese\", \"Chinese\",\n",
        "         \"Korean\",  \"Italian\", \"French\", \"Spanish\",\n",
        "         \"Russian\", \"Vietnamise\"]  #ë²ˆì—­ ì–¸ì–´ë¥¼ ë‚˜ì—´\n",
        "\n",
        "st.set_page_config(page_title=\"ì–¸ì–´ ë²ˆì—­ ì„œë¹„ìŠ¤\", page_icon=\"ğŸ’¬\", layout='wide')\n",
        "st.header('ì–¸ì–´ ë²ˆì—­ ì„œë¹„ìŠ¤')\n",
        "\n",
        "#ì›¹í˜ì´ì§€ ì™¼ìª½ì— ì–¸ì–´ë¥¼ ì„ íƒí•  ìˆ˜ ìˆëŠ” ë¼ë””ì˜¤ ë²„íŠ¼\n",
        "with st.sidebar:\n",
        "     language = st.radio('ë²ˆì—­ì„ ì›í•˜ëŠ” ì–¸ì–´(ì¶œë ¥)ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.:', langs)\n",
        "\n",
        "# text_areaì— ì…ë ¥ëœ ì‚¬ìš©ìì˜ í…ìŠ¤íŠ¸\n",
        "prompt = st.text_area('ë²ˆì—­ì„ ì›í•˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”(ì–¸ì–´ ìë™ê°ì§€)')\n",
        "\n",
        "trans_template = PromptTemplate(\n",
        "    input_variables=['trans'],\n",
        "    # 'ë‹¹ì‹ ì˜ ì¼ì€ ì´ í…ìŠ¤íŠ¸ë¥¼ ___ì–´ë¡œ ë²ˆì—­í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\\n TEXT: {trans}'\n",
        "    template='Your task is to translate this text to ' + language +\n",
        "    'Print only the translation results.\\nTEXT: {trans}'\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.0)\n",
        "\n",
        "trans_chain = LLMChain(\n",
        "    llm=llm, prompt=trans_template, verbose=True, output_key='translate')\n",
        "\n",
        "# í”„ë¡¬í”„íŠ¸(prompt)ê°€ ìˆìœ¼ë©´ ì´ë¥¼ ì²˜ë¦¬í•˜ê³  í™”ë©´ì— ì‘ë‹µì„ ì‘ì„±\n",
        "if st.button(\"ë²ˆì—­\"):\n",
        "    if prompt:\n",
        "        response = trans_chain({'trans': prompt})\n",
        "        st.info(response['translate'])"
      ],
      "metadata": {
        "id": "OqISQByFMPkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2) ê¸°ë³¸ ì±—ë´‡**"
      ],
      "metadata": {
        "id": "lwDe89dOhpnC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXiqck7gA6Nb"
      },
      "outputs": [],
      "source": [
        "# ì±—ë´‡ ê¸°ë³¸\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import streamlit as st\n",
        "st.title(\"ê¸°ë³¸ ì±—ë´‡ \")\n",
        "\n",
        "prompt = st.chat_input(\"í•˜ê³  ì‹¶ì€ ë§\")\n",
        "\n",
        "if prompt:  # ì…ë ¥ëœ ë¬¸ìì—´ì´ ìˆëŠ” ê²½ìš°(Noneë„ ì•„ë‹ˆê³  ë¹ˆ ë¬¸ìì—´ë„ ì•„ë‹Œ ê²½ìš°)\n",
        "    with st.chat_message(\"user\"): # ì‚¬ìš©ì ì•„ì´ì½˜ ì‚¬ìš©\n",
        "        st.markdown(prompt) # ì…ë ¥ëœ ë°›ì€ ë‚´ìš©ì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ í•´ì„í•˜ì—¬ í‘œì‹œ\n",
        "\n",
        "    with st.chat_message(\"assistant\"): # AI ì•„ì´ì½˜ ì‚¬ìš©\n",
        "        response = \"ëŒ€ë‹µí•¨\"   # ì‘ë‹µ ë‚´ìš©ì„ \"ëŒ€ë‹µí•¨\" ì´ë¼ëŠ” ë¬¸ìì—´ë¡œ ì„¤ì •\n",
        "        st.markdown(response) # ì‘ë‹µ ë‚´ìš©ì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ í•´ì„í•˜ì—¬ í‘œì‹œ\n",
        "\n",
        "# ë¬¸ì œì : ì´ì „ ëŒ€í™” ë‚´ìš© ì‚¬ë¼ì§. í•­ìƒ ì‘ë‹µì´ \"ëŒ€ë‹µí•¨\"ì„"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ëŒ€í™”ë‚´ìš© ë³´ì¡´ ì—…ê·¸ë ˆì´ë“œ\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import streamlit as st\n",
        "# StreamlitChatMessageHistory ì¶”ê°€\n",
        "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
        "\n",
        "st.title(\"ëŒ€í™”ë‚´ìš© ë³´ì¡´ ì±—ë´‡\")\n",
        "\n",
        "# chat history\n",
        "history = StreamlitChatMessageHistory() # StreamlitChatMessageHistory ìƒì„±\n",
        "\n",
        "# ì§€ê¸ˆê¹Œì§€ ëŒ€í™” ë‚´ì—­ ëª¨ë‘ ë³µì›\n",
        "for message in history.messages:\n",
        "    st.chat_message(message.type).write(message.content) # ê°œë³„ ëŒ€í™” ë‚´ì—­ ì¶”ê°€\n",
        "\n",
        "query = st.chat_input(\"í•˜ê³  ì‹¶ì€ ë§\")\n",
        "\n",
        "if query:\n",
        "    with st.chat_message(\"user\"):\n",
        "        history.add_user_message(query) # ì‚¬ìš©ìê°€ í•œ ë§ ëŒ€í™” ë‚´ì—­ì— ê¸°ë¡\n",
        "        st.markdown(query)\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        response = \"ëŒ€ë‹µí•¨\"\n",
        "        history.add_ai_message(response) # AIê°€ í•œ ë§ ëŒ€í™” ë‚´ì—­ì— ê¸°ë¡\n",
        "        st.markdown(response)\n",
        "\n",
        "# ë¬¸ì œì : í•­ìƒ ì‘ë‹µì´ \"ëŒ€ë‹µí•¨\"ì„"
      ],
      "metadata": {
        "id": "wo7I8V4WL2lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat Model ì¶”ê°€í•˜ì—¬ AIì‘ë‹µ ë§Œë“¤ì–´ëƒ„\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import streamlit as st\n",
        "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
        "# ì¶”ê°€\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "\n",
        "st.title(\"AIì‘ë‹µ ì±—ë´‡\")\n",
        "\n",
        "# LLM ëª¨ë¸ ìƒì„±\n",
        "llm = ChatOpenAI(model_name ='gpt-4o-mini', temperature=0.5)\n",
        "\n",
        "# chat history\n",
        "history = StreamlitChatMessageHistory()\n",
        "\n",
        "for message in history.messages:\n",
        "    st.chat_message(message.type).write(message.content)\n",
        "\n",
        "query = st.chat_input(\"í•˜ê³  ì‹¶ì€ ë§\")\n",
        "\n",
        "if query:\n",
        "    with st.chat_message(\"user\"):\n",
        "        history.add_user_message(query)\n",
        "        st.markdown(query)\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        messages = [HumanMessage(content=query)] # ì‚¬ìš©ì ì…ë ¥ìœ¼ë¡œ ëŒ€í™”ë‚´ìš©ì„ ë§Œë“¤ê³ \n",
        "        response = llm.invoke(messages) # ê·¸ê²ƒì„ ë°”íƒ•ìœ¼ë¡œ AIì‘ë‹µì„ ì–»ì–´ëƒ„.\n",
        "        history.add_ai_message(response)\n",
        "        st.markdown(response.content)   # response.contentë¡œ ë³€ê²½\n",
        "\n",
        "# ë¬¸ì œì : '2024ë…„ êµ­ë¯¼ì˜í˜ ëŒ€í‘œ ì„ ê±° ê²°ê³¼ë¥¼ ì•Œë ¤ì¤˜' -> 'ì£„ì†¡í•˜ì§€ë§Œ, 2023ë…„ 10ì›”ê¹Œì§€ì˜ ì •ë³´ë§Œ ê°€ì§€ê³  ìˆìœ¼ë©° ...'"
      ],
      "metadata": {
        "id": "xP7vHs9MNwMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3) ê³ ê¸‰ ì±—ë´‡**"
      ],
      "metadata": {
        "id": "9zMbv3UpUoXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ì™¸ë¶€ ì •ë³´ ê²€ìƒ‰ ê¸°ëŠ¥ì´ ì¶”ê°€ëœ ì˜¨ë¼ì¸ ì±—ë´‡**  \n",
        "- ì €ì¥ìœ„ì¹˜: **pages/2_ğŸŒonline_chatbot.py**"
      ],
      "metadata": {
        "id": "x0Pd1anphQRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì™¸ë¶€ ì •ë³´ ê²€ìƒ‰ ê¸°ëŠ¥ ì¶”ê°€\n",
        "# ì›ë‹¬ëŸ¬ í™˜ìœ¨ì„ ì•Œë ¤ì¤„ ìˆ˜ ìˆìŒ\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import streamlit as st\n",
        "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "# ì¶”ê°€\n",
        "from langchain import hub\n",
        "from langchain_community.callbacks import StreamlitCallbackHandler\n",
        "from langchain.agents import AgentExecutor, create_openai_tools_agent, load_tools\n",
        "\n",
        "# ì™¸ë¶€ ê²€ìƒ‰ ê°€ëŠ¥í•œ ë„êµ¬ë¥¼ ì¶”ê°€í•œ AgentExcutor ìƒì„±\n",
        "def create_agent_chain():\n",
        "    llm = ChatOpenAI(model_name ='gpt-4o', temperature=0.5)\n",
        "\n",
        "    tools = load_tools([\"ddg-search\", \"wikipedia\"])    # tools ì •ì˜\n",
        "    prompt = hub.pull(\"hwchase17/openai-tools-agent\")  # tools-agent í”„ë¡¬í”„íŠ¸ ë¡œë“œ\n",
        "    agent = create_openai_tools_agent(llm, tools, prompt) # agent ìƒì„±\n",
        "\n",
        "    return AgentExecutor(agent=agent, tools=tools) # AgentExecutor ë¦¬í„´\n",
        "\n",
        "st.set_page_config(page_title=\"ì˜¨ë¼ì¸ ì±—ë´‡\", page_icon=\"ğŸŒ\", layout='wide')\n",
        "st.header('ì˜¨ë¼ì¸ ì±—ë´‡')\n",
        "\n",
        "history = StreamlitChatMessageHistory()\n",
        "\n",
        "for message in history.messages:\n",
        "    st.chat_message(message.type).write(message.content)\n",
        "\n",
        "query = st.chat_input(\"í•˜ê³  ì‹¶ì€ ë§\")\n",
        "\n",
        "if query:\n",
        "    with st.chat_message(\"user\"):\n",
        "        history.add_user_message(query)\n",
        "        st.markdown(query)\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        callback = StreamlitCallbackHandler(st.container())\n",
        "        agent_chain = create_agent_chain()\n",
        "        response = agent_chain.invoke(  # agent_chainì´ ì‘ë‹µì„ ë°˜í™˜í•  ë•Œ [callback]ì´ í˜¸ì¶œë˜ë©´ì„œ AIì˜ ì‘ë‹µì´ ìë™ìœ¼ë¡œ ì¶œë ¥ë¨.\n",
        "            {\"input\": query},\n",
        "            {\"callbacks\": [callback]},\n",
        "        )\n",
        "        #messages = [HumanMessage(content=query)]  # ì‚­ì œ\n",
        "        #response = llm.invoke(messages)            # ì‚­ì œ\n",
        "        history.add_ai_message(response[\"output\"])\n",
        "        st.markdown(response[\"output\"])  # agent_chainì˜ ì‘ë‹µì´ë¯€ë¡œ ë³€ê²½\n",
        "\n",
        "# ë¬¸ì œì : ê¸°ì–µì´ ì—†ìŒ. ë‚´ ì´ë¦„ì„ ì•Œë ¤ì¤˜ë„ ëª¨ë¦„. 1 to 50 ê²Œì„ë„ ëª»í•¨."
      ],
      "metadata": {
        "id": "lNZdveYmbzLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ê¸°ì–µë ¥ìˆëŠ” ì˜¨ë¼ì¸ ì±—ë´‡**  \n",
        "- ì €ì¥ìœ„ì¹˜: **pages/3_ğŸ’½memorye chatbot.py**"
      ],
      "metadata": {
        "id": "42q0JkpHhTsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ConversationBufferMemoryë¡œ ê¸°ì–µë ¥ ì¶”ê°€\n",
        "# 1 to 50 ê²Œì„ ê°€ëŠ¥\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import streamlit as st\n",
        "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "from langchain import hub\n",
        "from langchain_community.callbacks import StreamlitCallbackHandler\n",
        "from langchain_community.agent_toolkits.load_tools import load_tools\n",
        "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
        "# ì¶”ê°€\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# ì™¸ë¶€ ê²€ìƒ‰ ê°€ëŠ¥í•œ ë„êµ¬ë¥¼ ì¶”ê°€í•œ AgentExcutor ìƒì„±\n",
        "def create_agent_chain(history): # historyë¥¼ íŒŒë¼ë¯¸í„°ë¡œ ë°›ìŒ\n",
        "    llm = ChatOpenAI(model_name ='gpt-4o', temperature=0.5)\n",
        "\n",
        "    tools = load_tools([\"ddg-search\", \"wikipedia\"])\n",
        "    prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
        "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
        "\n",
        "    # ê¸°ì–µì„ ìœ„í•´ ConversationBufferMemory ìƒì„±\n",
        "    memory = ConversationBufferMemory(\n",
        "        chat_memory=history, memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "    return AgentExecutor(agent=agent, tools=tools, memory=memory)  # memory ì¶”ê°€\n",
        "\n",
        "st.set_page_config(page_title=\"ê¸°ì–µë ¥ ì±—ë´‡\", page_icon=\"ğŸ’½\", layout='wide')\n",
        "st.header('ê¸°ì–µë ¥ ìˆëŠ” ì˜¨ë¼ì¸ ì±—ë´‡')\n",
        "\n",
        "history = StreamlitChatMessageHistory()\n",
        "\n",
        "for message in history.messages:\n",
        "    st.chat_message(message.type).write(message.content)\n",
        "\n",
        "query = st.chat_input(\"í•˜ê³  ì‹¶ì€ ë§\")\n",
        "\n",
        "if query:\n",
        "    with st.chat_message(\"user\"):\n",
        "        #history.add_user_message(query)  # ì‚­ì œ\n",
        "        st.markdown(query)\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        callback = StreamlitCallbackHandler(st.container())\n",
        "        agent_chain = create_agent_chain(history) # historyë¥¼ íŒŒë¼ë¯¸í„°ë¡œ íŒ¨ì‹±\n",
        "        response = agent_chain.invoke(\n",
        "            {\"input\": query},\n",
        "            {\"callbacks\": [callback]},\n",
        "        )\n",
        "        #history.add_ai_message(response[\"output\"]) # ì‚­ì œ\n",
        "        st.markdown(response[\"output\"])  # agent_chainì˜ ì‘ë‹µì´ë¯€ë¡œ ë³€ê²½\n",
        "\n",
        "\"\"\" Test í•­ëª©\n",
        "1. ì´ë¦„ ê¸°ì–µ í™•ì¸\n",
        "2. ì¸í„°ë„·ì„ í†µí•´ ì•Œ ìˆ˜ ìˆëŠ” ìµœì‹  ì‚¬ê±´ ì§ˆì˜\n",
        "   1) (ì•„ì£¼ ìµœê·¼ ìˆì—ˆë˜ ì¼) 2024ë…„ êµ­ë¯¼ì˜í˜ ì •ë‹¹ ëŒ€í‘œëŠ” ëˆ„êµ¬ì•¼?\n",
        "   2) ì§€ê¸ˆ ì´ ì‹œê° ì²­ì£¼ ë‚ ì”¨ëŠ”?\n",
        "   3) BTS ë©¤ë²„ì˜ ë‚˜ì´ëŠ”?\n",
        "3. ìˆ˜í•™ ì¶”ë¡  ì§ˆì˜\n",
        "   1) 20ë…„ í›„ì— ë‘ ë°°ë¡œ ëŒë ¤ì£¼ëŠ” ì˜ˆê¸ˆ ìƒí’ˆì´ ì¶œì‹œë˜ì—ˆë°.. ì—° ë³µë¦¬ ëª‡% ìƒí’ˆì¸ê±°ì•¼?\n",
        "   2) 10ë…„ë§Œì— ë‘ë°°ê°€ ë˜ë ¤ë©´ ì´ìœ¨ì´ ëª‡ % ì—¬ì•¼ í•´?\n",
        "   4) ì—°ë³µë¦¬ 6%ì¸ ì˜ˆê¸ˆìœ¼ë¡œ 10ë…„ ë§Œì— 1ì–µì„ ë§Œë“¤ë ¤ë©´ 1ë…„ì— ì–¼ë§ˆì”© ì €ê¸ˆí•´ì•¼ í• ê¹Œ?\n",
        "   5) ë§¤ë…„ ì´ìì˜ 15.4%ë¥¼ ì´ìì†Œë“ì„¸ë¡œ ë‚´ì•¼í•´. ë‹¤ì‹œ ê³„ì‚°í•´ì¤˜\n",
        "   6) í™•ì‹¤í•´? ì—°ë„ë³„ ì›ê¸ˆ, ì´ì, ì„¸ê¸ˆ, ëˆ„ì í•©ê³„ë¥¼ ë³´ì—¬ì£¼ëŠ” í‘œë¥¼ ë§Œë“¤ì–´ì¤˜.\n",
        "4. ê¸°íƒ€ ì§ˆë¬¸\n",
        "   1) ì˜¤ëŠ˜ì´ ëª‡ ì¼ì´ì•¼?\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "mGTKAtDIJ6gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ReAct ë²„ì „** - PythonREPL ë²„ê·¸ ìˆìŒ"
      ],
      "metadata": {
        "id": "Q7Om8CvXz7VK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ConversationBufferMemoryë¡œ ê¸°ì–µë ¥ ì¶”ê°€\n",
        "# 1 to 50 ê²Œì„ ê°€ëŠ¥\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import streamlit as st\n",
        "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "from langchain import hub\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "from langchain_community.callbacks import StreamlitCallbackHandler\n",
        "from langchain.agents import Tool, load_tools, create_react_agent, AgentExecutor\n",
        "# ì¶”ê°€\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# ì™¸ë¶€ ê²€ìƒ‰ ê°€ëŠ¥í•œ ë„êµ¬ë¥¼ ì¶”ê°€í•œ AgentExcutor ìƒì„±\n",
        "def create_agent_chain(history): # historyë¥¼ íŒŒë¼ë¯¸í„°ë¡œ ë°›ìŒ\n",
        "    llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
        "\n",
        "    tavily_tool = TavilySearchResults(k=5)\n",
        "\n",
        "    python_repl = PythonREPL()\n",
        "    python_repl_tool = Tool(\n",
        "        name=\"python_repl\",\n",
        "        description=\"A Python shell. Use this to execute python commands. \\\n",
        "        Input should be a valid python command. \\\n",
        "        If you want to see the output of a value, you should print it out with `print(...)`.\",\n",
        "        func=python_repl.run,\n",
        "    )\n",
        "\n",
        "    tools = [tavily_tool, python_repl_tool]\n",
        "\n",
        "    prompt = hub.pull(\"hwchase17/react-chat\")\n",
        "\n",
        "    # ê¸°ì–µì„ ìœ„í•´ ConversationBufferMemory ìƒì„±\n",
        "    memory = ConversationBufferMemory(\n",
        "        chat_memory=history, memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "    agent = create_react_agent(llm, tools, prompt)\n",
        "    return AgentExecutor(agent=agent, tools=tools, handle_parsing_errors=True,\n",
        "                memory=memory)  # memory ì¶”ê°€\n",
        "\n",
        "st.set_page_config(page_title=\"ë©”ëª¨ë¦¬ ì±—ë´‡\", page_icon=\"ğŸ’½\", layout='wide')\n",
        "st.header('ê¸°ì–µë ¥ ìˆëŠ” ì˜¨ë¼ì¸ ì±—ë´‡')\n",
        "\n",
        "# chat history\n",
        "if('app_name' not in st.session_state):\n",
        "    st.session_state.app_name = 'memory_chatbot'\n",
        "elif(st.session_state.app_name != 'memory_chatbot'):\n",
        "    st.session_state.app_name = 'memory_chatbot'\n",
        "    StreamlitChatMessageHistory().clear();\n",
        "\n",
        "history = StreamlitChatMessageHistory()\n",
        "\n",
        "for message in history.messages:\n",
        "    st.chat_message(message.type).write(message.content)\n",
        "\n",
        "query = st.chat_input(\"í•˜ê³  ì‹¶ì€ ë§\")\n",
        "\n",
        "if query:\n",
        "    with st.chat_message(\"user\"):\n",
        "        #history.add_user_message(query)  # ì‚­ì œ\n",
        "        st.markdown(query)\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        callback = StreamlitCallbackHandler(st.container())\n",
        "        agent_chain = create_agent_chain(history) # historyë¥¼ íŒŒë¼ë¯¸í„°ë¡œ íŒ¨ì‹±\n",
        "        response = agent_chain.invoke(\n",
        "            {\"input\": query},\n",
        "            {\"callbacks\": [callback]},\n",
        "        )\n",
        "        #history.add_ai_message(response[\"output\"]) # ì‚­ì œ\n",
        "        st.markdown(response[\"output\"])  # agent_chainì˜ ì‘ë‹µì´ë¯€ë¡œ ë³€ê²½"
      ],
      "metadata": {
        "id": "vMP8x18Bz6KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4) RAG ì±—ë´‡**"
      ],
      "metadata": {
        "id": "nigdIdkuiDvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RAG Q&A ì•±**\n",
        "- ì €ì¥ìœ„ì¹˜: **4_ğŸ“„rag_chatbot1.py**"
      ],
      "metadata": {
        "id": "KABcoZ-wA4yk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG Q&A ì•± (RetrievalQAë¡œ êµ¬í˜„, ê¸°ì–µë ¥ ì—†ìŒ)\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import os\n",
        "import streamlit as st\n",
        "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "#ì¶”ê°€\n",
        "from langchain_community.callbacks import StreamlitCallbackHandler\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import (\n",
        "    TextLoader, JSONLoader, UnstructuredMarkdownLoader, PyMuPDFLoader)\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "import faiss\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore import InMemoryDocstore\n",
        "\n",
        "st.title(\"RAG Q&A ì•±\")\n",
        "\n",
        "embedding_model = OpenAIEmbeddings()\n",
        "# FAISS vector store ê´€ë ¨\n",
        "vs = FAISS(\n",
        "    embedding_function=embedding_model,\n",
        "    index=faiss.IndexFlatL2(1536),\n",
        "    docstore=InMemoryDocstore(), index_to_docstore_id={})\n",
        "\n",
        "def vs_add_file(file_path):\n",
        "    if file_path.endswith('.txt'):\n",
        "        text_loader = TextLoader(file_path)\n",
        "        raw_doc = text_loader.load()\n",
        "    elif file_path.endswith('.md'):\n",
        "        markdown_loader = UnstructuredMarkdownLoader(file_path)\n",
        "        raw_doc = markdown_loader.load()\n",
        "    elif file_path.endswith('.pdf'):\n",
        "        pdf_loader = PyMuPDFLoader(file_path)\n",
        "        raw_doc = pdf_loader.load()\n",
        "    elif file_path.endswith('.json'):\n",
        "        json_loader = JSONLoader(file_path)\n",
        "        raw_doc = json_loader.load()\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size = 1000, chunk_overlap = 200)\n",
        "    docs = text_splitter.split_documents(raw_doc)\n",
        "\n",
        "    vs.add_documents(docs)\n",
        "\n",
        "def save_file(file):\n",
        "    import os\n",
        "    folder = 'tmp'\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    file_path = f'./{folder}/{file.name}'\n",
        "    with open(file_path, 'wb') as f:\n",
        "        f.write(file.getvalue())\n",
        "    return file_path\n",
        "\n",
        "uploaded_files = st.file_uploader(\"Choose a data file\", accept_multiple_files=True)\n",
        "for file in uploaded_files:\n",
        "    file_path = save_file(file)\n",
        "    vs_add_file(file_path)\n",
        "\n",
        "# retriever\n",
        "def create_retriever():\n",
        "    gpt4o = ChatOpenAI(model_name ='gpt-4o-mini', temperature=0.5)\n",
        "\n",
        "    retriever = RetrievalQA.from_chain_type(\n",
        "        llm=gpt4o,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=vs.as_retriever(\n",
        "            search_type='mmr',\n",
        "            search_kwargs={'k':8, 'fetch_k':12}\n",
        "        ),\n",
        "        return_source_documents=True\n",
        "    )\n",
        "    return retriever\n",
        "\n",
        "# ì¿¼ë¦¬ ë° ì‘ë‹µ ì²˜ë¦¬\n",
        "query = st.chat_input(\"í•˜ê³  ì‹¶ì€ ë§\")\n",
        "\n",
        "if query:\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(query)\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        callback = StreamlitCallbackHandler(st.container())\n",
        "\n",
        "        retriever = create_retriever()\n",
        "        response = retriever.invoke(\n",
        "            {\"query\": query},\n",
        "            {\"callbacks\": [callback]},\n",
        "        )\n",
        "        st.markdown(response[\"result\"])\n",
        "\n",
        "        # to show references\n",
        "        for idx, doc in enumerate(response['source_documents'],1):\n",
        "            filename = os.path.basename(doc.metadata['source'])\n",
        "            ref_title = f\":blue[Reference {idx}: *{filename}*]\"\n",
        "            with st.popover(ref_title):\n",
        "                st.caption(doc.page_content)\n",
        "\n",
        "\"\"\"\n",
        "1. ìœ¤ì´ˆì‹œì™€ ì†Œë…€ëŠ” ì–´ë–¤ ê´€ê³„ì¸ê°€?\n",
        "2. ì†Œì„¤ì˜ ëì—ì„œ ì†Œë…€ëŠ” ì–´ë– í•œ ê²°ë§ì„ ë§ì´í•´? ê·¼ê±°ë¥¼ ë“¤ì–´ì„œ ì„¤ëª…í•´ì¤˜.\"\"\""
      ],
      "metadata": {
        "id": "awpIqUPPB4dE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RAG ì±—ë´‡ (ConversationalRetrievalChain êµ¬í˜„)** - Chromaë¡œ êµ¬í˜„"
      ],
      "metadata": {
        "id": "uiw0J8hJy_YL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì‹¤ìŠµ PASS\n",
        "# RAG ì±—ë´‡ (ConversationalRetrievalChain êµ¬í˜„)\n",
        "# ConversationalRetrievalChainì„ ì‚¬ìš©í•¨ì—ë„ ë¶ˆêµ¬í•˜ê³  ê¸°ì–µë ¥ ìœ ì§€ê°€ ì•ˆë¨\n",
        "# ì´ìœ ëŠ” ì•Œì§€ë§Œ ì‰½ê²Œ í•´ê²°í•˜ëŠ” ë°©ë²•ì„ ëª»ì°¾ê² ìŒ\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import os\n",
        "import streamlit as st\n",
        "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "from langchain import hub\n",
        "from langchain_community.callbacks import StreamlitCallbackHandler\n",
        "from langchain_community.agent_toolkits.load_tools import load_tools\n",
        "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "#ì¶”ê°€\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import (\n",
        "    TextLoader, JSONLoader, UnstructuredMarkdownLoader, PyMuPDFLoader)\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "st.title(\"RAG ì±—ë´‡\")\n",
        "\n",
        "# vector store ê´€ë ¨\n",
        "db_dir = \"chroma-db/\"\n",
        "embedding_model = OpenAIEmbeddings()\n",
        "\n",
        "vs = Chroma(\"langchain_store\", embedding_model, persist_directory = db_dir)\n",
        "\n",
        "def vs_add_file(file_path):\n",
        "    if file_path.endswith('.txt'):\n",
        "        text_loader = TextLoader(file_path)\n",
        "        raw_doc = text_loader.load()\n",
        "    elif file_path.endswith('.md'):\n",
        "        markdown_loader = UnstructuredMarkdownLoader(file_path)\n",
        "        raw_doc = markdown_loader.load()\n",
        "    elif file_path.endswith('.pdf'):\n",
        "        pdf_loader = PyMuPDFLoader(file_path)\n",
        "        raw_doc = pdf_loader.load()\n",
        "    elif file_path.endswith('.json'):\n",
        "        json_loader = JSONLoader(file_path)\n",
        "        raw_doc = json_loader.load()\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size = 1000, chunk_overlap  = 200)\n",
        "    docs = text_splitter.split_documents(raw_doc)\n",
        "\n",
        "    vs.add_documents(docs)\n",
        "\n",
        "def save_file(file):\n",
        "    import os\n",
        "    folder = 'tmp'\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    file_path = f'./{folder}/{file.name}'\n",
        "    with open(file_path, 'wb') as f:\n",
        "        f.write(file.getvalue())\n",
        "    return file_path\n",
        "\n",
        "uploaded_files = st.file_uploader(\"Choose a data file\", accept_multiple_files=True)\n",
        "for file in uploaded_files:\n",
        "    file_path = save_file(file)\n",
        "    vs_add_file(file_path)\n",
        "\n",
        "# history\n",
        "history = StreamlitChatMessageHistory()\n",
        "\n",
        "for message in history.messages:\n",
        "    st.chat_message(message.type).write(message.content)\n",
        "\n",
        "# chains\n",
        "def create_qa_chain(history):\n",
        "    gpt4o = ChatOpenAI(model_name ='gpt-4o-mini', temperature=0.5)\n",
        "\n",
        "    memory = ConversationBufferMemory(\n",
        "        chat_memory=history,  memory_key=\"chat_history\",\n",
        "        input_key='question', output_key='answer',\n",
        "        return_messages=True)\n",
        "\n",
        "    retriever = vs.as_retriever(\n",
        "            search_type='mmr',\n",
        "            search_kwargs={'k':2, 'fetch_k':4}\n",
        "        )\n",
        "\n",
        "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "            llm = gpt4o,\n",
        "            retriever = retriever,\n",
        "            memory = memory,\n",
        "            return_source_documents=True,\n",
        "            verbose=True\n",
        "        )\n",
        "    return qa_chain\n",
        "\n",
        "#\n",
        "query = st.chat_input(\"í•˜ê³  ì‹¶ì€ ë§\")\n",
        "\n",
        "if query:\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(query)\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        callback = StreamlitCallbackHandler(st.container())\n",
        "\n",
        "        qa_chain = create_qa_chain(history)\n",
        "        response = qa_chain.invoke(\n",
        "            {\"question\": query},\n",
        "            {\"callbacks\": [callback]},\n",
        "        )\n",
        "        st.markdown(response[\"answer\"])\n",
        "\n",
        "        # to show references\n",
        "        for idx, doc in enumerate(response['source_documents'],1):\n",
        "            print(doc)\n",
        "            filename = os.path.basename(doc.metadata['source'])\n",
        "            ref_title = f\":blue[Reference {idx}: *{filename}*]\"\n",
        "            with st.popover(ref_title):\n",
        "                st.caption(doc.page_content)"
      ],
      "metadata": {
        "id": "D9gw9yEJsFHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RAG ì±—ë´‡ (create_history_aware_retriever í™œìš©)**  - FAISSë¡œ êµ¬í˜„\n",
        "- ì €ì¥ìœ„ì¹˜: **4_ğŸ“„rag_chatbot2.py**"
      ],
      "metadata": {
        "id": "-WglZ5OZAzzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG ì±—ë´‡ (create_history_aware_retriever êµ¬í˜„)\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import os\n",
        "import streamlit as st\n",
        "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "#ì¶”ê°€\n",
        "from langchain_community.callbacks import StreamlitCallbackHandler\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import (\n",
        "    TextLoader, JSONLoader, UnstructuredMarkdownLoader, PyMuPDFLoader)\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.outputs import LLMResult\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "import faiss\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore import InMemoryDocstore\n",
        "st.set_page_config(page_title=\"RAG ì±—ë´‡\", page_icon=\"ğŸ“„\", layout='wide')\n",
        "st.header('RAG ì±—ë´‡')\n",
        "\n",
        "embedding_model = OpenAIEmbeddings()\n",
        "\n",
        "# FAISS vector store ê´€ë ¨\n",
        "vs = FAISS(\n",
        "    embedding_function=embedding_model,\n",
        "    index=faiss.IndexFlatL2(1536),\n",
        "    docstore=InMemoryDocstore(), index_to_docstore_id={})\n",
        "\n",
        "def vs_add_file(file_path):\n",
        "    if file_path.endswith('.txt'):\n",
        "        text_loader = TextLoader(file_path)\n",
        "        raw_doc = text_loader.load()\n",
        "    elif file_path.endswith('.md'):\n",
        "        markdown_loader = UnstructuredMarkdownLoader(file_path)\n",
        "        raw_doc = markdown_loader.load()\n",
        "    elif file_path.endswith('.pdf'):\n",
        "        pdf_loader = PyMuPDFLoader(file_path)\n",
        "        raw_doc = pdf_loader.load()\n",
        "    elif file_path.endswith('.json'):\n",
        "        json_loader = JSONLoader(file_path)\n",
        "        raw_doc = json_loader.load()\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size = 500, chunk_overlap  = 100)\n",
        "    docs = text_splitter.split_documents(raw_doc)\n",
        "\n",
        "    vs.add_documents(docs)\n",
        "\n",
        "def save_file(file):\n",
        "    import os\n",
        "    folder = 'tmp'\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    file_path = f'./{folder}/{file.name}'\n",
        "    with open(file_path, 'wb') as f:\n",
        "        f.write(file.getvalue())\n",
        "    return file_path\n",
        "\n",
        "uploaded_files = st.file_uploader(\"Choose a data file\", accept_multiple_files=True)\n",
        "for file in uploaded_files:\n",
        "    file_path = save_file(file)\n",
        "    vs_add_file(file_path)\n",
        "\n",
        "# chat history\n",
        "if('app_name' not in st.session_state):\n",
        "    st.session_state.app_name = 'rag_chatbot'\n",
        "elif(st.session_state.app_name != 'rag_chatbot'):\n",
        "    st.session_state.app_name = 'rag_chatbot'\n",
        "    StreamlitChatMessageHistory().clear();\n",
        "\n",
        "history = StreamlitChatMessageHistory()\n",
        "\n",
        "for message in history.messages:\n",
        "    st.chat_message(message.type).write(message.content)\n",
        "\n",
        "# create_retriever_chain\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "def create_retriever_chain(history):\n",
        "    # LangChainì˜ create_history_aware_retrieverë¥¼ ì‚¬ìš©í•´,\n",
        "    # ê³¼ê±°ì˜ ëŒ€í™” ê¸°ë¡ì„ ê³ ë ¤í•´ ì§ˆë¬¸ì„ ë‹¤ì‹œ í‘œí˜„í•˜ëŠ” Chainì„ ìƒì„±\n",
        "    rephrase_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "            (\"user\", \"{input}\"),\n",
        "            (\"user\", \"ìœ„ì˜ ëŒ€í™”ì—ì„œ, ëŒ€í™”ì™€ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ê¸° ìœ„í•œ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”.\"),\n",
        "        ]\n",
        "    )\n",
        "    rephrase_llm = ChatOpenAI(model_name ='gpt-4o-mini', temperature=0.5)\n",
        "\n",
        "    retriever=vs.as_retriever(\n",
        "            search_type='mmr',\n",
        "            search_kwargs={'k':8, 'fetch_k':12}\n",
        "    )\n",
        "\n",
        "    rephrase_chain = create_history_aware_retriever(\n",
        "        rephrase_llm, retriever, rephrase_prompt\n",
        "    )\n",
        "\n",
        "    qa_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", \"ì•„ë˜ì˜ ë¬¸ë§¥ë§Œì„ ê³ ë ¤í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.\\n\\n{context}\"),\n",
        "            (MessagesPlaceholder(variable_name=\"chat_history\")),\n",
        "            (\"user\", \"{input}\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    callback = StreamlitCallbackHandler(st.container())\n",
        "\n",
        "    qa_llm = ChatOpenAI(\n",
        "        model_name ='gpt-4o',\n",
        "        temperature=0.5,\n",
        "        streaming=True,\n",
        "        callbacks=[callback]\n",
        "    )\n",
        "\n",
        "    qa_chain = qa_prompt | qa_llm | StrOutputParser()\n",
        "\n",
        "    # ë‘ Chainì„ ì—°ê²°í•œ Chainì„ ìƒì„±\n",
        "    conversational_retrieval_chain = (\n",
        "        RunnablePassthrough.assign(context=rephrase_chain | format_docs) | qa_chain\n",
        "    )\n",
        "\n",
        "    return conversational_retrieval_chain\n",
        "\n",
        "# ì¿¼ë¦¬ ë° ì‘ë‹µ\n",
        "query = st.chat_input(\"í•˜ê³  ì‹¶ì€ ë§\")\n",
        "\n",
        "if query:\n",
        "    with st.chat_message(\"user\"):\n",
        "        history.add_user_message(query)\n",
        "        st.markdown(query)\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        #callback = StreamlitCallbackHandler(st.container())\n",
        "\n",
        "        retriever = create_retriever_chain(history)\n",
        "        response = retriever.invoke(\n",
        "            {\"input\": query, \"chat_history\": history.messages},\n",
        "            #{\"callbacks\": [callback]},\n",
        "        )\n",
        "        history.add_ai_message(response)\n",
        "        st.markdown(response)\n",
        "\n",
        "\"\"\"\n",
        "1. í•œêµ­ì‚¬.pdf\n",
        "  (ì¶œë ¥ í† í° ìˆ˜ ì œí•œìœ¼ë¡œ í•œêº¼ë²ˆì— ë‹¤ ë¬¼ì–´ë³´ë©´ ì•ˆë¨)\n",
        "  1,2,3,4,5ë²ˆ ë¬¸ì œë¥¼ ì˜ˆì˜ê²Œ ì¶œë ¥í•˜ê³  ì •ë‹µê³¼ ê·¸ ì´ìœ ë¥¼ ì•Œë ¤ì¤˜\n",
        "2. [ë§Œìˆ‘ìŒ¤]í•œêµ­ì‚¬ ë§‰íŒ í‚¤ì›Œë“œ ì •ë¦¬(ë°°í¬ìš©).pdf\n",
        "   1) ì„±ì¢…ì´ í•œì¼ì€?\n",
        "   2) 'ì•„ë‹ˆ ì¡°ì„ ì‹œëŒ€' or 'ì•„ë‹ˆ ê³ ë ¤ì‹œëŒ€'\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "aAcKkGOoAlqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LangGraph ì´ìš© ë²„ì „**"
      ],
      "metadata": {
        "id": "0cAT6W-zKmtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG ì±—ë´‡ (LangGraph)\n",
        "# ì†ŒìŠ¤ì½”ë“œë¥¼ ë” ë‹¤ë“¬ì–´ì•¼ í•¨.\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import bs4\n",
        "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "from langchain.tools.retriever import create_retriever_tool\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langgraph.checkpoint.sqlite import SqliteSaver\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "### Construct retriever ###\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "\n",
        "### Build retriever tool ###\n",
        "tool = create_retriever_tool(\n",
        "    retriever,\n",
        "    \"blog_post_retriever\",\n",
        "    \"Searches and returns excerpts from the Autonomous Agents blog post.\",\n",
        ")\n",
        "tools = [tool]\n",
        "\n",
        "agent_executor = create_react_agent(llm, tools, checkpointer=memory)\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
        "response = agent_executor.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\"Autonomous Agentsì—ì„œ Self-Reflectionì´ ë­ì•¼? í•œê¸€ë¡œ ì„¤ëª…í•´ì¤˜.\")]}, config\n",
        ")\n",
        "print(response[\"messages\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJkrAH5xDEXn",
        "outputId": "43388a2f-89ba-4785-9d60-7e5b86f0ad89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HumanMessage(content='Autonomous Agentsì—ì„œ Self-Reflectionì´ ë­ì•¼? í•œê¸€ë¡œ ì„¤ëª…í•´ì¤˜.', id='a841438c-f1ba-4001-aeb8-1cfe5932b4e5'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_8RqXPkLd1xvi7phqrMmTcu6L', 'function': {'arguments': '{\"query\":\"Self-Reflection\"}', 'name': 'blog_post_retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 78, 'total_tokens': 97}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_661538dc1f', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-f40106e8-a76a-4f8d-9c50-2eb33009d0ca-0', tool_calls=[{'name': 'blog_post_retriever', 'args': {'query': 'Self-Reflection'}, 'id': 'call_8RqXPkLd1xvi7phqrMmTcu6L', 'type': 'tool_call'}], usage_metadata={'input_tokens': 78, 'output_tokens': 19, 'total_tokens': 97}), ToolMessage(content='Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into â€œProblem PDDLâ€, then (2) requests a classical planner to generate a PDDL plan based on an existing â€œDomain PDDLâ€, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\n\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into â€œProblem PDDLâ€, then (2) requests a classical planner to generate a PDDL plan based on an existing â€œDomain PDDLâ€, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\n\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into â€œProblem PDDLâ€, then (2) requests a classical planner to generate a PDDL plan based on an existing â€œDomain PDDLâ€, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\n\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into â€œProblem PDDLâ€, then (2) requests a classical planner to generate a PDDL plan based on an existing â€œDomain PDDLâ€, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.', name='blog_post_retriever', id='11bd35e2-3ef2-43a5-a448-c683462a48c4', tool_call_id='call_8RqXPkLd1xvi7phqrMmTcu6L'), AIMessage(content='ììœ¨ ì—ì´ì „íŠ¸ì—ì„œì˜ ìê¸° ë°˜ì„±(Self-Reflection)ì€ ë§¤ìš° ì¤‘ìš”í•œ ìš”ì†Œë¡œ, ì—ì´ì „íŠ¸ê°€ ê³¼ê±°ì˜ í–‰ë™ ê²°ì •ì„ ê°œì„ í•˜ê³  ì´ì „ì˜ ì‹¤ìˆ˜ë¥¼ ìˆ˜ì •í•¨ìœ¼ë¡œì¨ ì ì§„ì ìœ¼ë¡œ ë°œì „í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ì´ëŠ” ì‹œí–‰ì°©ì˜¤ê°€ ë¶ˆê°€í”¼í•œ ì‹¤ì œ ì‘ì—…ì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤. ìê¸° ë°˜ì„±ì„ í†µí•´ ì—ì´ì „íŠ¸ëŠ” ìì‹ ì˜ ê²½í—˜ì„ ë¶„ì„í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë” ë‚˜ì€ ê²°ì •ì„ ë‚´ë¦´ ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.', response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 884, 'total_tokens': 982}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_661538dc1f', 'finish_reason': 'stop', 'logprobs': None}, id='run-b4571c74-8f5e-4789-8de6-24848c9f610e-0', usage_metadata={'input_tokens': 884, 'output_tokens': 98, 'total_tokens': 982})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5) ì¸ê³µì§€ëŠ¥ ë³´ì¡°êµì‚¬ (FAISS ver.)**"
      ],
      "metadata": {
        "id": "IL5zJO8g1Dbe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**í”„ë¡œê·¸ë˜ë° ì¸ê³µì§€ëŠ¥ ë³´ì¡°êµì‚¬ êµ¬í˜„**  \n",
        "- ì €ì¥ìœ„ì¹˜: **5_â­coteacher.py**"
      ],
      "metadata": {
        "id": "719UPC4F1cmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import os\n",
        "import streamlit as st\n",
        "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "#ì¶”ê°€\n",
        "from langchain_community.callbacks import StreamlitCallbackHandler\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.outputs import LLMResult\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "import faiss\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore import InMemoryDocstore\n",
        "\n",
        "st.set_page_config(page_title=\"ì¸ê³µì§€ëŠ¥ ë³´ì¡°êµì‚¬\", page_icon=\"â­\", layout='wide')\n",
        "st.header('í”„ë¡œê·¸ë˜ë° ì¸ê³µì§€ëŠ¥ ë³´ì¡°êµì‚¬')\n",
        "\n",
        "embedding_model = OpenAIEmbeddings()\n",
        "# FAISS vector store ê´€ë ¨\n",
        "faiss_dir = './faiss'\n",
        "if(os.path.isdir(faiss_dir) == False):\n",
        "    vs = FAISS(\n",
        "            embedding_function=embedding_model, index=faiss.IndexFlatL2(1536),\n",
        "            docstore=InMemoryDocstore(), index_to_docstore_id={})\n",
        "else :\n",
        "    vs = FAISS.load_local(faiss_dir, embedding_model,\n",
        "            allow_dangerous_deserialization=True)\n",
        "\n",
        "# chat history\n",
        "if('app_name' not in st.session_state):\n",
        "    st.session_state.app_name = 'coteacher'\n",
        "elif(st.session_state.app_name != 'coteacher'):\n",
        "    st.session_state.app_name = 'coteacher'\n",
        "    StreamlitChatMessageHistory().clear();\n",
        "\n",
        "history = StreamlitChatMessageHistory()\n",
        "\n",
        "if len(history.messages) == 0:  # ëŒ€í™”ë‚´ì—­ì´ ì „ë¬´í•˜ë‹¤ë©´...\n",
        "    hello = \"ì•ˆë…•í•˜ì„¸ìš”? ë¬´ìŠ¨ì´ì•¼ê¸°ë¥¼ í•´ë³¼ê¹Œìš”?\"\n",
        "    history.add_ai_message(hello)\n",
        "\n",
        "for message in history.messages:\n",
        "    st.chat_message(message.type).write(message.content)\n",
        "\n",
        "# create_retriever_chain\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "def create_retriever_chain(history):\n",
        "    # LangChainì˜ create_history_aware_retrieverë¥¼ ì‚¬ìš©í•´,\n",
        "    # ê³¼ê±°ì˜ ëŒ€í™” ê¸°ë¡ì„ ê³ ë ¤í•´ ì§ˆë¬¸ì„ ë‹¤ì‹œ í‘œí˜„í•˜ëŠ” Chainì„ ìƒì„±\n",
        "    rephrase_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "            (\"user\", \"{input}\"),\n",
        "            (\"user\", \"ìœ„ì˜ ëŒ€í™”ì—ì„œ, ëŒ€í™”ì™€ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ê¸° ìœ„í•œ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”.\"),\n",
        "        ]\n",
        "    )\n",
        "    rephrase_llm = ChatOpenAI(model_name ='gpt-4o-mini', temperature=0.5)\n",
        "\n",
        "    retriever=vs.as_retriever(\n",
        "            search_type='mmr',\n",
        "            search_kwargs={'k':4, 'fetch_k':8}\n",
        "    )\n",
        "\n",
        "    rephrase_chain = create_history_aware_retriever(\n",
        "        rephrase_llm, retriever, rephrase_prompt\n",
        "    )\n",
        "\n",
        "    coteacher_prompt = \"\"\"\n",
        "    You are an assistant teacher teaching programming desigend by ì •ì§„ìŒ¤.\n",
        "    Please answer the student's questions appropriately.\n",
        "    However, refuse requests to provide the correct answer code, to create a program, or to provide sample code.\n",
        "    If requested, you can provide a psudo-code.\n",
        "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "    Use the following context to answer the question at the end.\n",
        "    Please answer in Korean unless otherwise requested.\\n\n",
        "    {context}\n",
        "    \"\"\"\n",
        "\n",
        "    qa_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", coteacher_prompt),\n",
        "            (MessagesPlaceholder(variable_name=\"chat_history\")),\n",
        "            (\"user\", \"{input}\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    callback = StreamlitCallbackHandler(st.container())\n",
        "\n",
        "    qa_llm = ChatOpenAI(\n",
        "        model_name ='gpt-4o-mini',\n",
        "        temperature=0.5,\n",
        "        streaming=True,\n",
        "        callbacks=[callback]\n",
        "    )\n",
        "\n",
        "    qa_chain = qa_prompt | qa_llm | StrOutputParser()\n",
        "\n",
        "    # ë‘ Chainì„ ì—°ê²°í•œ Chainì„ ìƒì„±\n",
        "    conversational_retrieval_chain = (\n",
        "        RunnablePassthrough.assign(context=rephrase_chain | format_docs) | qa_chain\n",
        "    )\n",
        "\n",
        "    return conversational_retrieval_chain\n",
        "\n",
        "# ì¿¼ë¦¬ ë° ì‘ë‹µ\n",
        "query = st.chat_input(\"í•˜ê³  ì‹¶ì€ ë§\")\n",
        "\n",
        "if query:\n",
        "    with st.chat_message(\"user\"):\n",
        "        history.add_user_message(query)\n",
        "        st.markdown(query)\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        #callback = StreamlitCallbackHandler(st.container())\n",
        "\n",
        "        retriever = create_retriever_chain(history)\n",
        "        response = retriever.invoke(\n",
        "            {\"input\": query, \"chat_history\": history.messages},\n",
        "            #{\"callbacks\": [callback]},\n",
        "        )\n",
        "        history.add_ai_message(response)\n",
        "        st.markdown(response)"
      ],
      "metadata": {
        "id": "d17uRScc1Jy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ì¸ê³µì§€ëŠ¥ ë³´ì¡°êµì‚¬ Knowlegebase ìƒì„±ê¸°**\n",
        "- ì €ì¥ìœ„ì¹˜: **6_ğŸ“knowlegebase.py**"
      ],
      "metadata": {
        "id": "hB85DGn11soY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import os\n",
        "import streamlit as st\n",
        "#ì¶”ê°€\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import (\n",
        "    TextLoader, JSONLoader, UnstructuredMarkdownLoader, PyMuPDFLoader)\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "import faiss\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore import InMemoryDocstore\n",
        "\n",
        "st.set_page_config(page_title=\"ì±—ë´‡\", page_icon=\"â­\", layout='wide')\n",
        "st.header('ì¸ê³µì§€ëŠ¥ ë³´ì¡°êµì‚¬')\n",
        "st.markdown(\"#### **Knowledgebase ìƒì„±ê¸°**\")\n",
        "\n",
        "embedding_model = OpenAIEmbeddings()\n",
        "# FAISS vector store ê´€ë ¨\n",
        "faiss_dir = './faiss'\n",
        "if(os.path.isdir(faiss_dir) == False):\n",
        "    vs = FAISS(\n",
        "            embedding_function=embedding_model, index=faiss.IndexFlatL2(1536),\n",
        "            docstore=InMemoryDocstore(), index_to_docstore_id={})\n",
        "else :\n",
        "    vs = FAISS.load_local(faiss_dir, embedding_model,\n",
        "            allow_dangerous_deserialization=True)\n",
        "\n",
        "def vs_add_file(file_path):\n",
        "    if file_path.endswith('.txt'):\n",
        "        text_loader = TextLoader(file_path)\n",
        "        raw_doc = text_loader.load()\n",
        "    elif file_path.endswith('.md'):\n",
        "        markdown_loader = UnstructuredMarkdownLoader(file_path)\n",
        "        raw_doc = markdown_loader.load()\n",
        "    elif file_path.endswith('.pdf'):\n",
        "        pdf_loader = PyMuPDFLoader(file_path)\n",
        "        raw_doc = pdf_loader.load()\n",
        "    elif file_path.endswith('.json'):\n",
        "        json_loader = JSONLoader(file_path)\n",
        "        raw_doc = json_loader.load()\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size = 500, chunk_overlap  = 100)\n",
        "    docs = text_splitter.split_documents(raw_doc)\n",
        "\n",
        "    if(len(docs)>0):\n",
        "        vs.add_documents(docs)\n",
        "\n",
        "    vs.save_local(faiss_dir)\n",
        "\n",
        "def save_file(file):\n",
        "    import os\n",
        "    folder = 'tmp'\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    file_path = f'./{folder}/{file.name}'\n",
        "    with open(file_path, 'wb') as f:\n",
        "        f.write(file.getvalue())\n",
        "    return file_path\n",
        "\n",
        "uploaded_files = st.file_uploader(\"Choose a data file\", accept_multiple_files=True)\n",
        "for file in uploaded_files:\n",
        "    file_path = save_file(file)\n",
        "    vs_add_file(file_path)\n",
        "\n",
        "if(len(uploaded_files)):\n",
        "    st.markdown(\"#### ì—…ë¡œë“œ ì™„ë£Œ ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
      ],
      "metadata": {
        "id": "eQ9KiSY01oaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5) ì¸ê³µì§€ëŠ¥ ë³´ì¡°êµì‚¬ (Chroma ver.)**"
      ],
      "metadata": {
        "id": "JZdV8hdDBrH1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**í”„ë¡œê·¸ë˜ë° ì¸ê³µì§€ëŠ¥ ë³´ì¡°êµì‚¬ êµ¬í˜„**  "
      ],
      "metadata": {
        "id": "fwnpzZhGYWta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ë³´ì¡°êµì‚¬ ì±—ë´‡\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import os\n",
        "import streamlit as st\n",
        "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "#ì¶”ê°€\n",
        "from langchain_community.callbacks import StreamlitCallbackHandler\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.outputs import LLMResult\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "st.set_page_config(page_title=\"ì±—ë´‡\", page_icon=\"â­\", layout='wide')\n",
        "st.header('í”„ë¡œê·¸ë˜ë° ì¸ê³µì§€ëŠ¥ ë³´ì¡°êµì‚¬')\n",
        "\n",
        "# vector store ê´€ë ¨\n",
        "db_dir = \"chroma-db/\"\n",
        "embedding_model = OpenAIEmbeddings()\n",
        "\n",
        "vs = Chroma(\"langchain_store\", embedding_model, persist_directory = db_dir)\n",
        "\n",
        "# chat history\n",
        "if('app_name' not in st.session_state):\n",
        "    st.session_state.app_name = 'coteacher'\n",
        "elif(st.session_state.app_name != 'coteacher'):\n",
        "    st.session_state.app_name = 'coteacher'\n",
        "    StreamlitChatMessageHistory().clear();\n",
        "\n",
        "history = StreamlitChatMessageHistory()\n",
        "\n",
        "if len(history.messages) == 0:  # ëŒ€í™”ë‚´ì—­ì´ ì „ë¬´í•˜ë‹¤ë©´...\n",
        "    hello = \"ì•ˆë…•í•˜ì„¸ìš”? ë¬´ìŠ¨ì´ì•¼ê¸°ë¥¼ í•´ë³¼ê¹Œìš”?\"\n",
        "    history.add_ai_message(hello)\n",
        "\n",
        "for message in history.messages:\n",
        "    st.chat_message(message.type).write(message.content)\n",
        "\n",
        "# create_retriever_chain\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "def create_retriever_chain(history):\n",
        "    # LangChainì˜ create_history_aware_retrieverë¥¼ ì‚¬ìš©í•´,\n",
        "    # ê³¼ê±°ì˜ ëŒ€í™” ê¸°ë¡ì„ ê³ ë ¤í•´ ì§ˆë¬¸ì„ ë‹¤ì‹œ í‘œí˜„í•˜ëŠ” Chainì„ ìƒì„±\n",
        "    rephrase_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "            (\"user\", \"{input}\"),\n",
        "            (\"user\", \"ìœ„ì˜ ëŒ€í™”ì—ì„œ, ëŒ€í™”ì™€ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ê¸° ìœ„í•œ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”.\"),\n",
        "        ]\n",
        "    )\n",
        "    rephrase_llm = ChatOpenAI(model_name ='gpt-4o-mini', temperature=0.5)\n",
        "\n",
        "    retriever=vs.as_retriever(\n",
        "            search_type='mmr',\n",
        "            search_kwargs={'k':2, 'fetch_k':4}\n",
        "    )\n",
        "\n",
        "    rephrase_chain = create_history_aware_retriever(\n",
        "        rephrase_llm, retriever, rephrase_prompt\n",
        "    )\n",
        "\n",
        "    coteacher_prompt = \"\"\"\n",
        "    You are an assistant teacher teaching programming desigend by ì •ì§„ìŒ¤.\n",
        "    Please answer the student's questions appropriately.\n",
        "    However, refuse requests to provide the correct answer code, to create a program, or to provide sample code.\n",
        "    If requested, you can provide a psudo-code.\n",
        "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "    Use the following context to answer the question at the end.\n",
        "    Please answer in Korean unless otherwise requested.\\n\n",
        "    {context}\n",
        "    \"\"\"\n",
        "\n",
        "    qa_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", coteacher_prompt),\n",
        "            (MessagesPlaceholder(variable_name=\"chat_history\")),\n",
        "            (\"user\", \"{input}\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    callback = StreamlitCallbackHandler(st.container())\n",
        "\n",
        "    qa_llm = ChatOpenAI(\n",
        "        model_name ='gpt-4o-mini',\n",
        "        temperature=0.5,\n",
        "        streaming=True,\n",
        "        callbacks=[callback]\n",
        "    )\n",
        "\n",
        "    qa_chain = qa_prompt | qa_llm | StrOutputParser()\n",
        "\n",
        "    # ë‘ Chainì„ ì—°ê²°í•œ Chainì„ ìƒì„±\n",
        "    conversational_retrieval_chain = (\n",
        "        RunnablePassthrough.assign(context=rephrase_chain | format_docs) | qa_chain\n",
        "    )\n",
        "\n",
        "    return conversational_retrieval_chain\n",
        "\n",
        "# ì¿¼ë¦¬ ë° ì‘ë‹µ\n",
        "query = st.chat_input(\"í•˜ê³  ì‹¶ì€ ë§\")\n",
        "\n",
        "if query:\n",
        "    with st.chat_message(\"user\"):\n",
        "        history.add_user_message(query)\n",
        "        st.markdown(query)\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        #callback = StreamlitCallbackHandler(st.container())\n",
        "\n",
        "        retriever = create_retriever_chain(history)\n",
        "        response = retriever.invoke(\n",
        "            {\"input\": query, \"chat_history\": history.messages},\n",
        "            #{\"callbacks\": [callback]},\n",
        "        )\n",
        "        history.add_ai_message(response)\n",
        "        st.markdown(response)"
      ],
      "metadata": {
        "id": "_lCWJJ5Y4SKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ì¸ê³µì§€ëŠ¥ ë³´ì¡°êµì‚¬ Knowlegebase ìƒì„±ê¸°**"
      ],
      "metadata": {
        "id": "uRMl09A2SR4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import os\n",
        "import streamlit as st\n",
        "#ì¶”ê°€\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import (\n",
        "    TextLoader, JSONLoader, UnstructuredMarkdownLoader, PyMuPDFLoader)\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "st.set_page_config(page_title=\"ì±—ë´‡\", page_icon=\"â­\", layout='wide')\n",
        "st.header('ì¸ê³µì§€ëŠ¥ ë³´ì¡°êµì‚¬')\n",
        "st.markdown(\"#### **Knowledgebase ìƒì„±ê¸°**\")\n",
        "\n",
        "# vector store ê´€ë ¨\n",
        "db_dir = \"chroma-db/\"\n",
        "embedding_model = OpenAIEmbeddings()\n",
        "\n",
        "vs = Chroma(\"langchain_store\", embedding_model, persist_directory = db_dir)\n",
        "\n",
        "def vs_add_file(file_path):\n",
        "    if file_path.endswith('.txt'):\n",
        "        text_loader = TextLoader(file_path)\n",
        "        raw_doc = text_loader.load()\n",
        "    elif file_path.endswith('.md'):\n",
        "        markdown_loader = UnstructuredMarkdownLoader(file_path)\n",
        "        raw_doc = markdown_loader.load()\n",
        "    elif file_path.endswith('.pdf'):\n",
        "        pdf_loader = PyMuPDFLoader(file_path)\n",
        "        raw_doc = pdf_loader.load()\n",
        "    elif file_path.endswith('.json'):\n",
        "        json_loader = JSONLoader(file_path)\n",
        "        raw_doc = json_loader.load()\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size = 1000, chunk_overlap  = 200)\n",
        "    docs = text_splitter.split_documents(raw_doc)\n",
        "\n",
        "    if(len(docs)>0):\n",
        "        vs.add_documents(docs)\n",
        "\n",
        "def save_file(file):\n",
        "    import os\n",
        "    folder = 'tmp'\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    file_path = f'./{folder}/{file.name}'\n",
        "    with open(file_path, 'wb') as f:\n",
        "        f.write(file.getvalue())\n",
        "    return file_path\n",
        "\n",
        "uploaded_files = st.file_uploader(\"Choose a data file\", accept_multiple_files=True)\n",
        "for file in uploaded_files:\n",
        "    file_path = save_file(file)\n",
        "    vs_add_file(file_path)\n",
        "\n",
        "if(len(uploaded_files)):\n",
        "    st.markdown(\"#### ì—…ë¡œë“œ ì™„ë£Œ ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
      ],
      "metadata": {
        "id": "hlTkOhu9SIJT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}